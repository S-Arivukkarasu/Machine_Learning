{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ae3bdf8-21e1-4c67-8ca3-6bbac4b3396c",
   "metadata": {},
   "source": [
    "# Building the UI using Gradio\n",
    "Today we will build User Interfaces using the outrageously simple Gradio framework.\n",
    "\n",
    "Please note: your Gradio screens may appear in 'dark mode' or 'light mode' depending on your computer settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d74e8ca-6482-4c7c-a83e-1733e88650ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install if Gradio is not installed\n",
    "# !pip install --upgrade gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30680aed-7b98-467b-a5a1-2fa22e27930e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Libraries\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from typing import List\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from bs4 import BeautifulSoup\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fb72559-655a-4dd0-b2b5-a643c28a4a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a standard System prompt\n",
    "\n",
    "system_message = \"you are a helpful assistant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfa4c29f-624f-4f69-9b1f-85e9bbec7de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_URL = \"http://localhost:11434/api/chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5cc46d0-14db-49a1-8eba-a4c31f69a712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_mistral_ollama(prompt):\n",
    "    MODEL_NAME=\"mistral\"  # Get this from you local ollama by using \"ollama list\" if Mistral not available do ollama run mistral\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": 0.8,\n",
    "        \"stream\": True  # Important: to get streaming responses\n",
    "    }\n",
    "    start_time = time.time()\n",
    "    response = requests.post(OLLAMA_URL, json=payload, stream=True)\n",
    "    # reply = response.json()[\"message\"][\"content\"]\n",
    "    # reply = re.sub(r\"<think>.*?</think>\", \"\", reply, flags=re.DOTALL).strip()\n",
    "    # print(\"Total Time Taken: \", time.time()-start_time, \"\\n\\n\")\n",
    "    # return reply\n",
    "    result=\"\"\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            try:\n",
    "                data = json.loads(line.decode('utf-8'))\n",
    "                delta = data.get(\"message\", {}).get(\"content\", \"\")\n",
    "                if delta:\n",
    "                    # Optional: remove unwanted <think> tags or others\n",
    "                    clean_delta = re.sub(r\"<think>.*?</think>\", \"\", delta, flags=re.DOTALL)\n",
    "                    result += clean_delta\n",
    "                    yield result\n",
    "            except json.JSONDecodeError:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc680477-1b6d-4879-98f0-4c7dfbb6f42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Today\n",
      " Today'\n",
      " Today's\n",
      " Today's date\n",
      " Today's date,\n",
      " Today's date, according\n",
      " Today's date, according to\n",
      " Today's date, according to my\n",
      " Today's date, according to my system\n",
      " Today's date, according to my system,\n",
      " Today's date, according to my system, is\n",
      " Today's date, according to my system, is March\n",
      " Today's date, according to my system, is March \n",
      " Today's date, according to my system, is March 2\n",
      " Today's date, according to my system, is March 24\n",
      " Today's date, according to my system, is March 24,\n",
      " Today's date, according to my system, is March 24, \n",
      " Today's date, according to my system, is March 24, 2\n",
      " Today's date, according to my system, is March 24, 20\n",
      " Today's date, according to my system, is March 24, 202\n",
      " Today's date, according to my system, is March 24, 2023\n",
      " Today's date, according to my system, is March 24, 2023.\n",
      " Today's date, according to my system, is March 24, 2023. However\n",
      " Today's date, according to my system, is March 24, 2023. However,\n",
      " Today's date, according to my system, is March 24, 2023. However, please\n",
      " Today's date, according to my system, is March 24, 2023. However, please verify\n",
      " Today's date, according to my system, is March 24, 2023. However, please verify this\n",
      " Today's date, according to my system, is March 24, 2023. However, please verify this information\n",
      " Today's date, according to my system, is March 24, 2023. However, please verify this information as\n",
      " Today's date, according to my system, is March 24, 2023. However, please verify this information as I\n",
      " Today's date, according to my system, is March 24, 2023. However, please verify this information as I am\n",
      " Today's date, according to my system, is March 24, 2023. However, please verify this information as I am unable\n",
      " Today's date, according to my system, is March 24, 2023. However, please verify this information as I am unable to\n",
      " Today's date, according to my system, is March 24, 2023. However, please verify this information as I am unable to access\n",
      " Today's date, according to my system, is March 24, 2023. However, please verify this information as I am unable to access real\n",
      " Today's date, according to my system, is March 24, 2023. However, please verify this information as I am unable to access real-\n",
      " Today's date, according to my system, is March 24, 2023. However, please verify this information as I am unable to access real-time\n",
      " Today's date, according to my system, is March 24, 2023. However, please verify this information as I am unable to access real-time data\n",
      " Today's date, according to my system, is March 24, 2023. However, please verify this information as I am unable to access real-time data and\n",
      " Today's date, according to my system, is March 24, 2023. However, please verify this information as I am unable to access real-time data and my\n",
      " Today's date, according to my system, is March 24, 2023. However, please verify this information as I am unable to access real-time data and my knowledge\n",
      " Today's date, according to my system, is March 24, 2023. However, please verify this information as I am unable to access real-time data and my knowledge cut\n",
      " Today's date, according to my system, is March 24, 2023. However, please verify this information as I am unable to access real-time data and my knowledge cutoff\n",
      " Today's date, according to my system, is March 24, 2023. However, please verify this information as I am unable to access real-time data and my knowledge cutoff is\n",
      " Today's date, according to my system, is March 24, 2023. However, please verify this information as I am unable to access real-time data and my knowledge cutoff is \n",
      " Today's date, according to my system, is March 24, 2023. However, please verify this information as I am unable to access real-time data and my knowledge cutoff is 2\n",
      " Today's date, according to my system, is March 24, 2023. However, please verify this information as I am unable to access real-time data and my knowledge cutoff is 20\n",
      " Today's date, according to my system, is March 24, 2023. However, please verify this information as I am unable to access real-time data and my knowledge cutoff is 202\n",
      " Today's date, according to my system, is March 24, 2023. However, please verify this information as I am unable to access real-time data and my knowledge cutoff is 2021\n",
      " Today's date, according to my system, is March 24, 2023. However, please verify this information as I am unable to access real-time data and my knowledge cutoff is 2021.\n"
     ]
    }
   ],
   "source": [
    "for data in call_mistral_ollama(\"what is todays date\"):\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7758a068-158e-4f96-b865-6d49f36b76b0",
   "metadata": {},
   "source": [
    "## User Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2acae07b-7d66-48d4-a96e-5080e776beda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shout(text):\n",
    "    return text.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6638e817-793f-4750-80bd-b77cdd514c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HELLO'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shout(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15b4b5a3-ee1b-4f83-8e3b-e92c19855b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://67fe73947b7cbaacf9.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://67fe73947b7cbaacf9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view = gr.Interface(fn=shout, inputs=\"textbox\", outputs=\"textbox\")\n",
    "view.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7708fcd6-be3d-47a4-bb33-e8726baa6a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view = gr.Interface(\n",
    "    fn=call_mistral_ollama,\n",
    "    inputs=[gr.Textbox(label=\"Your Messages:\", lines=8)],\n",
    "    outputs=[gr.Textbox(label=\"Reponse:\", lines=8)],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce3312e3-bc83-43f4-8c19-7410566e5536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_message = \"You are a helpful assistant that responds in markdown\"\n",
    "\n",
    "view = gr.Interface(\n",
    "    fn=call_mistral_ollama,\n",
    "    inputs=[gr.Textbox(label=\"Your Messages:\")],\n",
    "    outputs=[gr.Markdown(label=\"Reponse:\")],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0071502f-0135-41cb-b36f-2d64d6eeb392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llama_ollama(prompt):\n",
    "    MODEL_NAME=\"llama3.1\"  # Get this from you local ollama by using \"ollama list\"\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": 0.8,\n",
    "        \"stream\": True  # Important: to get streaming responses\n",
    "    }\n",
    "    start_time = time.time()\n",
    "    response = requests.post(OLLAMA_URL, json=payload, stream=True)\n",
    "    # reply = response.json()[\"message\"][\"content\"]\n",
    "    # reply = re.sub(r\"<think>.*?</think>\", \"\", reply, flags=re.DOTALL).strip()\n",
    "    # print(\"Total Time Taken: \", time.time()-start_time, \"\\n\\n\")\n",
    "    # return reply\n",
    "    result=\"\"\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            try:\n",
    "                data = json.loads(line.decode('utf-8'))\n",
    "                delta = data.get(\"message\", {}).get(\"content\", \"\")\n",
    "                if delta:\n",
    "                    # Optional: remove unwanted <think> tags or others\n",
    "                    clean_delta = re.sub(r\"<think>.*?</think>\", \"\", delta, flags=re.DOTALL)\n",
    "                    result += clean_delta\n",
    "                    yield result\n",
    "            except json.JSONDecodeError:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd96c088-0069-43e8-9f37-c984ff53b71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view = gr.Interface(\n",
    "    fn=call_llama_ollama,\n",
    "    inputs=[gr.Textbox(label=\"Your Messages:\")],\n",
    "    outputs=[gr.Markdown(label=\"Reponse:\")],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8f20bb2-7329-4742-8b89-36c50d7766df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model(prompt, model):\n",
    "    if model == \"Llama\":\n",
    "        result = call_llama_ollama(prompt)\n",
    "    elif model == \"Mistral\":\n",
    "        result = call_mistral_ollama(prompt)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown Model\")\n",
    "    yield from result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b33da507-0920-48bd-b905-25e8dedb4801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view = gr.Interface(\n",
    "    fn=select_model,\n",
    "    inputs=[gr.Textbox(label=\"Your Messages:\"), gr.Dropdown([\"Llama\", \"Mistral\"], label=\"Select Model\")],\n",
    "    outputs=[gr.Markdown(label=\"Reponse:\")],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3bf81b-d9fd-4b13-9c14-63256cb20b8c",
   "metadata": {},
   "source": [
    "# Building a company brochure generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76c0d487-0137-44c6-9db6-4bbe68b3f961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class to represent a Webpage\n",
    "\n",
    "# Some websites need you to use proper headers when fetching them:\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "class Website:\n",
    "    \"\"\"\n",
    "    A utility class to represent a Website that we have scraped, now with links\n",
    "    \"\"\"\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        response = requests.get(headers=headers, url=self.url)\n",
    "        self.body = response.content\n",
    "        soup = BeautifulSoup(self.body, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No Title Found\"\n",
    "        if soup.body:\n",
    "            for irrelevent in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "                irrelevent.decompose()\n",
    "            self.text = soup.body.get_text(separator='\\n', strip=True)\n",
    "        else:\n",
    "            self.text = \"\"\n",
    "\n",
    "        links = [link.get('href') for link in soup.find_all('a')]\n",
    "        self.links = [link for link in links if link]\n",
    "\n",
    "    def get_contents(self):\n",
    "        return f\"Webpage Title:\\n{self.title}\\nWebpage Contents:\\n{self.text}\\n\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f87d5b18-8c5a-4cc0-98a9-626afa3abb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that analyzes the contents of a company website landing page \\\n",
    "and creates a short brochure about the company for prospective customers, investors and recruits. Respond in markdown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19724733-c7a0-4908-add5-d3c1f4e2e8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_brochure(company_name, url, model):\n",
    "    yield \"\"\n",
    "    prompt = f\"Please generate a company brochure for {company_name}. Here is their landing page:\\n\"\n",
    "    prompt+=Website(url).get_contents()\n",
    "    if model == \"Llama\":\n",
    "        result = call_llama_ollama(prompt)\n",
    "    elif model == \"Mistral\":\n",
    "        result = call_mistral_ollama(prompt)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown Model\")\n",
    "    yield from result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53ef23f6-dcf9-46a2-acb9-5d2f9f7a1383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view = gr.Interface(\n",
    "    fn=stream_brochure,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Company Name:\"),\n",
    "        gr.Textbox(label=\"Landing page URL including http:// or https://\"),\n",
    "        gr.Dropdown([\"Llama\", \"Mistral\"], label=\"Select Model\")\n",
    "    ],\n",
    "    outputs=[gr.Markdown(label=\"Brochure:\")],\n",
    "    flagging_mode=\"never\",\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59a0230-4d70-4035-ac44-7c4ccf6dfc4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
