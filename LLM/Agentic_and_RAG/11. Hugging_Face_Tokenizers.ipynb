{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8e225f0-04d3-4c5c-9efd-ccdc24cacb06",
   "metadata": {},
   "source": [
    "# Welcome to Tokenizers and Models\n",
    "\n",
    "It's the lower level APIs which gives you more control and power over the Model\n",
    "\n",
    "## Tokenizers\n",
    "\n",
    "A **tokenizer** is responsible for breaking down text into smaller chunks, called tokens, that the model can understand. Models like GPT or BERT cannot directly work with raw text (like \"I love AI\"), so they need the text to be split into tokens, which are usually words or subwords. The tokenizer maps these tokens to numerical IDs (called token IDs) that the model can process.\n",
    "\n",
    "* Tokenization: Converting text into tokens.\n",
    "\n",
    "* Detokenization: Converting tokens back into human-readable text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b407cf60-fd34-4534-8e43-a094426a30f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a30fcfd-a454-4970-89f4-cda858b33164",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading HF Token\n",
    "\n",
    "hf_token = os.getenv(\"HUGGING_FACE_WRITE_TOKEN\")\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2fe76e-14d4-4e3b-88dd-bd61a66b1e48",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 1. Token\n",
    "- A **token** is a unit of text that the model processes, which can be a word, part of a word, or even punctuation.\n",
    "- Tokens are the basic building blocks of text that the model understands and works with.\n",
    "- For example: \"ChatGPT is awesome!\" might be split into tokens like ['ChatGPT', 'is', 'awesome', '!'].\n",
    "\n",
    "### 2. Token ID\n",
    "- A **token ID** is the unique numerical representation of a token in the model's vocabulary.\n",
    "- Token IDs are used by the model to process the tokens as numbers instead of raw text.\n",
    "- For example, in a vocabulary:\n",
    "    - The token \"ChatGPT\" might have a token ID of 342.\n",
    "    - The token \"is\" might have a token ID of 15.\n",
    "    - The token \"awesome\" might have a token ID of 789.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8c5937-faf7-4ec4-b2e7-53a5f133f54f",
   "metadata": {},
   "source": [
    "### Accessing Llama 3.1 from Meta\n",
    "\n",
    "In order to use the fantastic Llama 3.1, Meta does require you to sign their terms of service.\n",
    "\n",
    "Visit their model instructions page in Hugging Face:\n",
    "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B\n",
    "\n",
    "At the top of the page are instructions on how to agree to their terms. If possible, you should use the same email as your huggingface account.\n",
    "\n",
    "In my experience approval comes in a couple of minutes. Once you've been approved for any 3.1 model, it applies to the whole 3.1 family of models. For whatever reason, occasionally Meta doesn't approve access. If that happens to you, please follow [this](https://colab.research.google.com/drive/1deJO03YZTXUwcq2vzxWbiBhrRuI29Vo8?usp=sharing) troubleshooting.\n",
    "\n",
    "If the next cell gives you an error, then please check:  \n",
    "1. Are you logged in to HuggingFace? Try running `login()` to check your key works\n",
    "2. Did you set up your API key with full read and write permissions?\n",
    "3. If you visit the Llama3.1 page with the link above, does it show that you have access to the model near the top?\n",
    "\n",
    "I've also set up this troubleshooting colab to try to diagnose any HuggingFace connectivity issues:  \n",
    "https://colab.research.google.com/drive/1deJO03YZTXUwcq2vzxWbiBhrRuI29Vo8?usp=sharing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dafd859-6135-4b5a-abdb-ac775ca410aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e925532c5784714958a51338dab4650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afd233a6e15f493fb90497b9e0d11656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef57e1bdf60248728d9c27689a69a2b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3.1-8B', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf7a697b-2b3e-44f7-aba6-968ba8c1d65e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000,\n",
       " 40,\n",
       " 1097,\n",
       " 12304,\n",
       " 311,\n",
       " 1501,\n",
       " 9857,\n",
       " 12509,\n",
       " 304,\n",
       " 1957,\n",
       " 311,\n",
       " 856,\n",
       " 445,\n",
       " 11237,\n",
       " 25175]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I am excited to show Tokenizers in action to my LLM engineers\"\n",
    "# encode is the function which will convert the words into numbers\n",
    "tokens = tokenizer.encode(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1777339-376f-43b8-8be6-d42293c58abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 61 characters, 12 words and 15 tokens\n"
     ]
    }
   ],
   "source": [
    "character_count = len(text)\n",
    "word_count = len(text.split(' '))\n",
    "token_count = len(tokens)\n",
    "print(f\"There are {character_count} characters, {word_count} words and {token_count} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2719c48-46a2-47ae-9675-a99cfe9646c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>I am excited to show Tokenizers in action to my LLM engineers'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decode will return the encoded numbers to words with some special tokens \n",
    "\n",
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e91b94f9-b62f-474e-986c-6eb57ad6ee51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|>',\n",
       " 'I',\n",
       " ' am',\n",
       " ' excited',\n",
       " ' to',\n",
       " ' show',\n",
       " ' Token',\n",
       " 'izers',\n",
       " ' in',\n",
       " ' action',\n",
       " ' to',\n",
       " ' my',\n",
       " ' L',\n",
       " 'LM',\n",
       " ' engineers']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ecd1fb2-c143-4b37-b04f-920a6646d988",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<|begin_of_text|>': 128000,\n",
       " '<|end_of_text|>': 128001,\n",
       " '<|reserved_special_token_0|>': 128002,\n",
       " '<|reserved_special_token_1|>': 128003,\n",
       " '<|finetune_right_pad_id|>': 128004,\n",
       " '<|reserved_special_token_2|>': 128005,\n",
       " '<|start_header_id|>': 128006,\n",
       " '<|end_header_id|>': 128007,\n",
       " '<|eom_id|>': 128008,\n",
       " '<|eot_id|>': 128009,\n",
       " '<|python_tag|>': 128010,\n",
       " '<|reserved_special_token_3|>': 128011,\n",
       " '<|reserved_special_token_4|>': 128012,\n",
       " '<|reserved_special_token_5|>': 128013,\n",
       " '<|reserved_special_token_6|>': 128014,\n",
       " '<|reserved_special_token_7|>': 128015,\n",
       " '<|reserved_special_token_8|>': 128016,\n",
       " '<|reserved_special_token_9|>': 128017,\n",
       " '<|reserved_special_token_10|>': 128018,\n",
       " '<|reserved_special_token_11|>': 128019,\n",
       " '<|reserved_special_token_12|>': 128020,\n",
       " '<|reserved_special_token_13|>': 128021,\n",
       " '<|reserved_special_token_14|>': 128022,\n",
       " '<|reserved_special_token_15|>': 128023,\n",
       " '<|reserved_special_token_16|>': 128024,\n",
       " '<|reserved_special_token_17|>': 128025,\n",
       " '<|reserved_special_token_18|>': 128026,\n",
       " '<|reserved_special_token_19|>': 128027,\n",
       " '<|reserved_special_token_20|>': 128028,\n",
       " '<|reserved_special_token_21|>': 128029,\n",
       " '<|reserved_special_token_22|>': 128030,\n",
       " '<|reserved_special_token_23|>': 128031,\n",
       " '<|reserved_special_token_24|>': 128032,\n",
       " '<|reserved_special_token_25|>': 128033,\n",
       " '<|reserved_special_token_26|>': 128034,\n",
       " '<|reserved_special_token_27|>': 128035,\n",
       " '<|reserved_special_token_28|>': 128036,\n",
       " '<|reserved_special_token_29|>': 128037,\n",
       " '<|reserved_special_token_30|>': 128038,\n",
       " '<|reserved_special_token_31|>': 128039,\n",
       " '<|reserved_special_token_32|>': 128040,\n",
       " '<|reserved_special_token_33|>': 128041,\n",
       " '<|reserved_special_token_34|>': 128042,\n",
       " '<|reserved_special_token_35|>': 128043,\n",
       " '<|reserved_special_token_36|>': 128044,\n",
       " '<|reserved_special_token_37|>': 128045,\n",
       " '<|reserved_special_token_38|>': 128046,\n",
       " '<|reserved_special_token_39|>': 128047,\n",
       " '<|reserved_special_token_40|>': 128048,\n",
       " '<|reserved_special_token_41|>': 128049,\n",
       " '<|reserved_special_token_42|>': 128050,\n",
       " '<|reserved_special_token_43|>': 128051,\n",
       " '<|reserved_special_token_44|>': 128052,\n",
       " '<|reserved_special_token_45|>': 128053,\n",
       " '<|reserved_special_token_46|>': 128054,\n",
       " '<|reserved_special_token_47|>': 128055,\n",
       " '<|reserved_special_token_48|>': 128056,\n",
       " '<|reserved_special_token_49|>': 128057,\n",
       " '<|reserved_special_token_50|>': 128058,\n",
       " '<|reserved_special_token_51|>': 128059,\n",
       " '<|reserved_special_token_52|>': 128060,\n",
       " '<|reserved_special_token_53|>': 128061,\n",
       " '<|reserved_special_token_54|>': 128062,\n",
       " '<|reserved_special_token_55|>': 128063,\n",
       " '<|reserved_special_token_56|>': 128064,\n",
       " '<|reserved_special_token_57|>': 128065,\n",
       " '<|reserved_special_token_58|>': 128066,\n",
       " '<|reserved_special_token_59|>': 128067,\n",
       " '<|reserved_special_token_60|>': 128068,\n",
       " '<|reserved_special_token_61|>': 128069,\n",
       " '<|reserved_special_token_62|>': 128070,\n",
       " '<|reserved_special_token_63|>': 128071,\n",
       " '<|reserved_special_token_64|>': 128072,\n",
       " '<|reserved_special_token_65|>': 128073,\n",
       " '<|reserved_special_token_66|>': 128074,\n",
       " '<|reserved_special_token_67|>': 128075,\n",
       " '<|reserved_special_token_68|>': 128076,\n",
       " '<|reserved_special_token_69|>': 128077,\n",
       " '<|reserved_special_token_70|>': 128078,\n",
       " '<|reserved_special_token_71|>': 128079,\n",
       " '<|reserved_special_token_72|>': 128080,\n",
       " '<|reserved_special_token_73|>': 128081,\n",
       " '<|reserved_special_token_74|>': 128082,\n",
       " '<|reserved_special_token_75|>': 128083,\n",
       " '<|reserved_special_token_76|>': 128084,\n",
       " '<|reserved_special_token_77|>': 128085,\n",
       " '<|reserved_special_token_78|>': 128086,\n",
       " '<|reserved_special_token_79|>': 128087,\n",
       " '<|reserved_special_token_80|>': 128088,\n",
       " '<|reserved_special_token_81|>': 128089,\n",
       " '<|reserved_special_token_82|>': 128090,\n",
       " '<|reserved_special_token_83|>': 128091,\n",
       " '<|reserved_special_token_84|>': 128092,\n",
       " '<|reserved_special_token_85|>': 128093,\n",
       " '<|reserved_special_token_86|>': 128094,\n",
       " '<|reserved_special_token_87|>': 128095,\n",
       " '<|reserved_special_token_88|>': 128096,\n",
       " '<|reserved_special_token_89|>': 128097,\n",
       " '<|reserved_special_token_90|>': 128098,\n",
       " '<|reserved_special_token_91|>': 128099,\n",
       " '<|reserved_special_token_92|>': 128100,\n",
       " '<|reserved_special_token_93|>': 128101,\n",
       " '<|reserved_special_token_94|>': 128102,\n",
       " '<|reserved_special_token_95|>': 128103,\n",
       " '<|reserved_special_token_96|>': 128104,\n",
       " '<|reserved_special_token_97|>': 128105,\n",
       " '<|reserved_special_token_98|>': 128106,\n",
       " '<|reserved_special_token_99|>': 128107,\n",
       " '<|reserved_special_token_100|>': 128108,\n",
       " '<|reserved_special_token_101|>': 128109,\n",
       " '<|reserved_special_token_102|>': 128110,\n",
       " '<|reserved_special_token_103|>': 128111,\n",
       " '<|reserved_special_token_104|>': 128112,\n",
       " '<|reserved_special_token_105|>': 128113,\n",
       " '<|reserved_special_token_106|>': 128114,\n",
       " '<|reserved_special_token_107|>': 128115,\n",
       " '<|reserved_special_token_108|>': 128116,\n",
       " '<|reserved_special_token_109|>': 128117,\n",
       " '<|reserved_special_token_110|>': 128118,\n",
       " '<|reserved_special_token_111|>': 128119,\n",
       " '<|reserved_special_token_112|>': 128120,\n",
       " '<|reserved_special_token_113|>': 128121,\n",
       " '<|reserved_special_token_114|>': 128122,\n",
       " '<|reserved_special_token_115|>': 128123,\n",
       " '<|reserved_special_token_116|>': 128124,\n",
       " '<|reserved_special_token_117|>': 128125,\n",
       " '<|reserved_special_token_118|>': 128126,\n",
       " '<|reserved_special_token_119|>': 128127,\n",
       " '<|reserved_special_token_120|>': 128128,\n",
       " '<|reserved_special_token_121|>': 128129,\n",
       " '<|reserved_special_token_122|>': 128130,\n",
       " '<|reserved_special_token_123|>': 128131,\n",
       " '<|reserved_special_token_124|>': 128132,\n",
       " '<|reserved_special_token_125|>': 128133,\n",
       " '<|reserved_special_token_126|>': 128134,\n",
       " '<|reserved_special_token_127|>': 128135,\n",
       " '<|reserved_special_token_128|>': 128136,\n",
       " '<|reserved_special_token_129|>': 128137,\n",
       " '<|reserved_special_token_130|>': 128138,\n",
       " '<|reserved_special_token_131|>': 128139,\n",
       " '<|reserved_special_token_132|>': 128140,\n",
       " '<|reserved_special_token_133|>': 128141,\n",
       " '<|reserved_special_token_134|>': 128142,\n",
       " '<|reserved_special_token_135|>': 128143,\n",
       " '<|reserved_special_token_136|>': 128144,\n",
       " '<|reserved_special_token_137|>': 128145,\n",
       " '<|reserved_special_token_138|>': 128146,\n",
       " '<|reserved_special_token_139|>': 128147,\n",
       " '<|reserved_special_token_140|>': 128148,\n",
       " '<|reserved_special_token_141|>': 128149,\n",
       " '<|reserved_special_token_142|>': 128150,\n",
       " '<|reserved_special_token_143|>': 128151,\n",
       " '<|reserved_special_token_144|>': 128152,\n",
       " '<|reserved_special_token_145|>': 128153,\n",
       " '<|reserved_special_token_146|>': 128154,\n",
       " '<|reserved_special_token_147|>': 128155,\n",
       " '<|reserved_special_token_148|>': 128156,\n",
       " '<|reserved_special_token_149|>': 128157,\n",
       " '<|reserved_special_token_150|>': 128158,\n",
       " '<|reserved_special_token_151|>': 128159,\n",
       " '<|reserved_special_token_152|>': 128160,\n",
       " '<|reserved_special_token_153|>': 128161,\n",
       " '<|reserved_special_token_154|>': 128162,\n",
       " '<|reserved_special_token_155|>': 128163,\n",
       " '<|reserved_special_token_156|>': 128164,\n",
       " '<|reserved_special_token_157|>': 128165,\n",
       " '<|reserved_special_token_158|>': 128166,\n",
       " '<|reserved_special_token_159|>': 128167,\n",
       " '<|reserved_special_token_160|>': 128168,\n",
       " '<|reserved_special_token_161|>': 128169,\n",
       " '<|reserved_special_token_162|>': 128170,\n",
       " '<|reserved_special_token_163|>': 128171,\n",
       " '<|reserved_special_token_164|>': 128172,\n",
       " '<|reserved_special_token_165|>': 128173,\n",
       " '<|reserved_special_token_166|>': 128174,\n",
       " '<|reserved_special_token_167|>': 128175,\n",
       " '<|reserved_special_token_168|>': 128176,\n",
       " '<|reserved_special_token_169|>': 128177,\n",
       " '<|reserved_special_token_170|>': 128178,\n",
       " '<|reserved_special_token_171|>': 128179,\n",
       " '<|reserved_special_token_172|>': 128180,\n",
       " '<|reserved_special_token_173|>': 128181,\n",
       " '<|reserved_special_token_174|>': 128182,\n",
       " '<|reserved_special_token_175|>': 128183,\n",
       " '<|reserved_special_token_176|>': 128184,\n",
       " '<|reserved_special_token_177|>': 128185,\n",
       " '<|reserved_special_token_178|>': 128186,\n",
       " '<|reserved_special_token_179|>': 128187,\n",
       " '<|reserved_special_token_180|>': 128188,\n",
       " '<|reserved_special_token_181|>': 128189,\n",
       " '<|reserved_special_token_182|>': 128190,\n",
       " '<|reserved_special_token_183|>': 128191,\n",
       " '<|reserved_special_token_184|>': 128192,\n",
       " '<|reserved_special_token_185|>': 128193,\n",
       " '<|reserved_special_token_186|>': 128194,\n",
       " '<|reserved_special_token_187|>': 128195,\n",
       " '<|reserved_special_token_188|>': 128196,\n",
       " '<|reserved_special_token_189|>': 128197,\n",
       " '<|reserved_special_token_190|>': 128198,\n",
       " '<|reserved_special_token_191|>': 128199,\n",
       " '<|reserved_special_token_192|>': 128200,\n",
       " '<|reserved_special_token_193|>': 128201,\n",
       " '<|reserved_special_token_194|>': 128202,\n",
       " '<|reserved_special_token_195|>': 128203,\n",
       " '<|reserved_special_token_196|>': 128204,\n",
       " '<|reserved_special_token_197|>': 128205,\n",
       " '<|reserved_special_token_198|>': 128206,\n",
       " '<|reserved_special_token_199|>': 128207,\n",
       " '<|reserved_special_token_200|>': 128208,\n",
       " '<|reserved_special_token_201|>': 128209,\n",
       " '<|reserved_special_token_202|>': 128210,\n",
       " '<|reserved_special_token_203|>': 128211,\n",
       " '<|reserved_special_token_204|>': 128212,\n",
       " '<|reserved_special_token_205|>': 128213,\n",
       " '<|reserved_special_token_206|>': 128214,\n",
       " '<|reserved_special_token_207|>': 128215,\n",
       " '<|reserved_special_token_208|>': 128216,\n",
       " '<|reserved_special_token_209|>': 128217,\n",
       " '<|reserved_special_token_210|>': 128218,\n",
       " '<|reserved_special_token_211|>': 128219,\n",
       " '<|reserved_special_token_212|>': 128220,\n",
       " '<|reserved_special_token_213|>': 128221,\n",
       " '<|reserved_special_token_214|>': 128222,\n",
       " '<|reserved_special_token_215|>': 128223,\n",
       " '<|reserved_special_token_216|>': 128224,\n",
       " '<|reserved_special_token_217|>': 128225,\n",
       " '<|reserved_special_token_218|>': 128226,\n",
       " '<|reserved_special_token_219|>': 128227,\n",
       " '<|reserved_special_token_220|>': 128228,\n",
       " '<|reserved_special_token_221|>': 128229,\n",
       " '<|reserved_special_token_222|>': 128230,\n",
       " '<|reserved_special_token_223|>': 128231,\n",
       " '<|reserved_special_token_224|>': 128232,\n",
       " '<|reserved_special_token_225|>': 128233,\n",
       " '<|reserved_special_token_226|>': 128234,\n",
       " '<|reserved_special_token_227|>': 128235,\n",
       " '<|reserved_special_token_228|>': 128236,\n",
       " '<|reserved_special_token_229|>': 128237,\n",
       " '<|reserved_special_token_230|>': 128238,\n",
       " '<|reserved_special_token_231|>': 128239,\n",
       " '<|reserved_special_token_232|>': 128240,\n",
       " '<|reserved_special_token_233|>': 128241,\n",
       " '<|reserved_special_token_234|>': 128242,\n",
       " '<|reserved_special_token_235|>': 128243,\n",
       " '<|reserved_special_token_236|>': 128244,\n",
       " '<|reserved_special_token_237|>': 128245,\n",
       " '<|reserved_special_token_238|>': 128246,\n",
       " '<|reserved_special_token_239|>': 128247,\n",
       " '<|reserved_special_token_240|>': 128248,\n",
       " '<|reserved_special_token_241|>': 128249,\n",
       " '<|reserved_special_token_242|>': 128250,\n",
       " '<|reserved_special_token_243|>': 128251,\n",
       " '<|reserved_special_token_244|>': 128252,\n",
       " '<|reserved_special_token_245|>': 128253,\n",
       " '<|reserved_special_token_246|>': 128254,\n",
       " '<|reserved_special_token_247|>': 128255}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_added_vocab will give you the special tokens of the models\n",
    "\n",
    "tokenizer.get_added_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f691c46-4a09-4e6f-8d51-641c46a1c9cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.get_added_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1785bae-3946-45a2-a450-76c5bd4242aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128256"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab will give you the total dictionary of words the model trained on \n",
    "\n",
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b903f8-f891-41be-85f1-a6604685a773",
   "metadata": {},
   "source": [
    "```meta-llama/Meta-Llama-3.1-8B``` is the base model it can generate text but i will not help in a chat.\n",
    "\n",
    "For chat we have the instruct/chat variant of this same model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f2edb2-acb3-400f-b7d9-48618878f91c",
   "metadata": {},
   "source": [
    "#### Instruct variants of models\n",
    "\n",
    "Many models have a variant that has been trained for use in Chats.  \n",
    "These are typically labelled with the word \"Instruct\" at the end.  \n",
    "They have been trained to expect prompts with a particular format that includes system, user and assistant prompts.  \n",
    "\n",
    "There is a utility method `apply_chat_template` that will convert from the messages list format we are familiar with, into the right input prompt for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adc9699a-5307-4b40-9827-7de4b5a1d07e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36bf48ac14ea4b908b01f5e3b6f0dfec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e828b43b4e478896f375ed2d692143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1af4a1e05ca4599a99785e93b5482a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3.1-8B-Instruct', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7950070f-17cc-458c-9bd4-e837fc8c20dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Tell a light-hearted joke for a room of Data Scientists<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell a light-hearted joke for a room of Data Scientists\"}\n",
    "  ]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3263908-f18f-4ab2-bb6c-fa09f90574ef",
   "metadata": {},
   "source": [
    "### Crucial \"Aha\" moment\n",
    "\n",
    "The LLMs could receive a list of python dictionaries in some way:\n",
    "\n",
    "```python\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell a light-hearted joke for a room of Data Scientists\"}\n",
    "  ]\n",
    "```\n",
    "\n",
    "But an LLM is just a Data Science model that takes a sequence of numbers and predicts the probability of the next number! You can't pass a bunch of Python objects into a statistical model!\n",
    "\n",
    "### And now you have the missing piece of the puzzle..\n",
    "\n",
    "The messages in OpenAI format get converted:\n",
    "\n",
    "1. ...into a sequence of words with special tags to separate the System, User, Assistant prompt\n",
    "2. ...then the words are broken down into fragments - \"tokens\"\n",
    "3. ...then the tokens are replaced with Token IDs - and this is the input sequence\n",
    "\n",
    "> The input to an LLM is a sequence of Token IDs. The output is the probability distribution of the next Token ID to follow this input.\n",
    "\n",
    "That's it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846aca4b-39ec-497a-aca5-ac4235b4e7ff",
   "metadata": {},
   "source": [
    "### Trying new, powerful open source models\n",
    "\n",
    "We will now work with 3 models:\n",
    "\n",
    "Phi4 from Microsoft  \n",
    "DeepSeek 3.1 from DeepSeek AI  \n",
    "QwenCoder 2.5 from Alibaba Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "022758cf-8dd0-407a-9819-ec3d9d32a6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PHI4 = \"microsoft/Phi-4-mini-instruct\"\n",
    "DEEPSEEK = \"deepseek-ai/DeepSeek-V3.1\"\n",
    "QWEN_CODER = \"Qwen/Qwen2.5-Coder-7B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbfc3e6d-2777-493b-8675-5e32cab78f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama:\n",
      "[128000, 40, 1097, 2917, 13610, 12304, 311, 1501, 473, 36368, 19109, 9857, 12509, 304, 1957, 311, 856, 445, 11237, 25175]\n",
      "['<|begin_of_text|>', 'I', ' am', ' cur', 'iously', ' excited', ' to', ' show', ' H', 'ugging', ' Face', ' Token', 'izers', ' in', ' action', ' to', ' my', ' L', 'LM', ' engineers']\n",
      "\n",
      "Phi 4:\n",
      "[40, 939, 4396, 23138, 15209, 316, 2356, 59116, 4512, 29049, 17951, 24223, 306, 3736, 316, 922, 451, 19641, 32437]\n",
      "['I', ' am', ' cur', 'iously', ' excited', ' to', ' show', ' Hug', 'ging', ' Face', ' Token', 'izers', ' in', ' action', ' to', ' my', ' L', 'LM', ' engineers']\n"
     ]
    }
   ],
   "source": [
    "phi4_tokenizer = AutoTokenizer.from_pretrained(PHI4)\n",
    "\n",
    "text = \"I am curiously excited to show Hugging Face Tokenizers in action to my LLM engineers\"\n",
    "print(\"Llama:\")\n",
    "tokens = tokenizer.encode(text)\n",
    "print(tokens)\n",
    "print(tokenizer.batch_decode(tokens))\n",
    "print(\"\\nPhi 4:\")\n",
    "tokens = phi4_tokenizer.encode(text)\n",
    "print(tokens)\n",
    "print(phi4_tokenizer.batch_decode(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "353ee6e8-b194-4e76-bab5-ad168c844b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Tell a light-hearted joke for a room of Data Scientists<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Phi 4:\n",
      "<|system|>You are a helpful assistant<|end|><|user|>Tell a light-hearted joke for a room of Data Scientists<|end|><|assistant|>\n"
     ]
    }
   ],
   "source": [
    "print(\"Llama:\")\n",
    "print(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
    "print(\"\\nPhi 4:\")\n",
    "print(phi4_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "578e44ca-523b-49f3-9a49-3a5b9858fac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128000, 40, 1097, 2917, 13610, 12304, 311, 1501, 473, 36368, 19109, 9857, 12509, 304, 1957, 311, 856, 445, 11237, 25175]\n",
      "\n",
      "[40, 939, 4396, 23138, 15209, 316, 2356, 59116, 4512, 29049, 17951, 24223, 306, 3736, 316, 922, 451, 19641, 32437]\n",
      "\n",
      "[0, 43, 1030, 108771, 15046, 304, 1801, 24133, 5426, 11906, 47948, 24524, 295, 4271, 304, 1026, 33792, 47, 26170]\n"
     ]
    }
   ],
   "source": [
    "deepseek_tokenizer = AutoTokenizer.from_pretrained(DEEPSEEK)\n",
    "\n",
    "text = \"I am curiously excited to show Hugging Face Tokenizers in action to my LLM engineers\"\n",
    "print(tokenizer.encode(text))\n",
    "print()\n",
    "print(phi4_tokenizer.encode(text))\n",
    "print()\n",
    "print(deepseek_tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ceca671-8feb-4e11-8019-588a53431f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Tell a light-hearted joke for a room of Data Scientists<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Phi:\n",
      "<|system|>You are a helpful assistant<|end|><|user|>Tell a light-hearted joke for a room of Data Scientists<|end|><|assistant|>\n",
      "\n",
      "DeepSeek:\n",
      "<｜begin▁of▁sentence｜>You are a helpful assistant<｜User｜>Tell a light-hearted joke for a room of Data Scientists<｜Assistant｜></think>\n"
     ]
    }
   ],
   "source": [
    "print(\"Llama:\")\n",
    "print(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
    "print(\"\\nPhi:\")\n",
    "print(phi4_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
    "print(\"\\nDeepSeek:\")\n",
    "print(deepseek_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9f93a8e-ef1a-4376-972d-320c3fc1c790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198=\n",
      "\n",
      "750=def\n",
      "23811= hello\n",
      "31792=_world\n",
      "29766=(person\n",
      "982=):\n",
      "\n",
      "220= \n",
      "1173= print\n",
      "445=(\"\n",
      "9707=Hello\n",
      "497=\",\n",
      "1697= person\n",
      "340=)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qwen_tokenizer = AutoTokenizer.from_pretrained(QWEN_CODER)\n",
    "code = \"\"\"\n",
    "def hello_world(person):\n",
    "  print(\"Hello\", person)\n",
    "\"\"\"\n",
    "tokens = qwen_tokenizer.encode(code)\n",
    "for token in tokens:\n",
    "  print(f\"{token}={qwen_tokenizer.decode(token)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f114bf6-ddd2-4ddb-8cb5-e1b9f3463dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
