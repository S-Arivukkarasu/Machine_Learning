{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "961d4e6f-5823-4ac4-8487-73350d7faaff",
   "metadata": {},
   "source": [
    "# Langchain (Core Framework)\n",
    "\n",
    "Langchain is a powerful framework built to help developers create applications that utilize language models for complex workflows. It‚Äôs designed to make it easier to build chains of prompts, connect different models, and orchestrate these models for real-world tasks like chatbots, document processing, and data extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "431851ae-573e-4ee3-bc46-5b49e90a3aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Langchain if not\n",
    "# !pip install langchain\n",
    "# !pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b70caff4-a937-44e4-b5f6-18502b0ced2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display, update_display\n",
    "import time\n",
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30f18e11-b68d-42c5-87bb-97fc8b1ac876",
   "metadata": {},
   "outputs": [],
   "source": [
    "tell_a_joke = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell a joke for a student on the journey to becoming an expert in LLM Engineering\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcb786d6-6844-4197-a739-59691de56d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_99242/1381029127.py:1: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"gemma3\")\n"
     ]
    }
   ],
   "source": [
    "llm = Ollama(model=\"gemma3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0724e057-c365-4717-aca9-f465d35b37d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.353596687316895\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Okay, here's a joke for a student on the journey to becoming an LLM Engineering expert:\n",
       "\n",
       "Why did the LLM break up with the prompt? \n",
       "\n",
       "... Because it said, \"I need more context! You're not giving me enough *parameters*!\" \n",
       "\n",
       "---\n",
       "\n",
       "Hope that brought a smile to your face! Let me know if you'd like another one. üòä \n",
       "\n",
       "Do you want a joke related to a specific aspect of LLM engineering (like fine-tuning, prompt engineering, or evaluation)?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "response_langchan = llm.invoke(tell_a_joke)\n",
    "print(time.time() - start_time)\n",
    "display(Markdown(response_langchan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5ab3c4a-afd4-4260-bea3-dfa653c9d2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.chat_models import ChatOllama\n",
    "# from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# from langchain.memory import ConversationBufferMemory\n",
    "# from langchain.chains import LLMChain\n",
    "\n",
    "# llm = ChatOllama(model=\"gemma3\")\n",
    "\n",
    "# memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", \"You are a friendly assistant who remembers context.\"),\n",
    "#     MessagesPlaceholder(variable_name=\"history\"),\n",
    "#     (\"human\", \"{message}\")\n",
    "# ])\n",
    "\n",
    "# chain = LLMChain(\n",
    "#     llm=llm,\n",
    "#     prompt=prompt,\n",
    "#     memory=memory\n",
    "# )\n",
    "\n",
    "# print(chain.invoke({\"message\": \"Hi, my name is John.\"}))\n",
    "# print(chain.invoke({\"message\": \"What is my name?\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "399f4db8-b955-4fda-973e-023878e59245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.chat_models import ChatOllama\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# llm = ChatOllama(model=\"gemma3\")\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", \"You are a witty comedian.\"),\n",
    "#     (\"human\", \"{question}\")\n",
    "# ])\n",
    "\n",
    "# chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# print(chain.invoke({\"question\": \"Tell me a joke.\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72ce7637-5db3-45e1-a2eb-1c05012671e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.chat_models import ChatOllama\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# class OllamaChatChain:\n",
    "#     def __init__(self, model_name=\"gemma3\", system_prompt=\"You are a helpful assistant.\"):\n",
    "#         # LLM\n",
    "#         self.llm = ChatOllama(model=model_name)\n",
    "\n",
    "#         # Prompt template with system + human messages\n",
    "#         self.prompt = ChatPromptTemplate.from_messages([\n",
    "#             (\"system\", system_prompt),\n",
    "#             (\"human\", \"{user_input}\")\n",
    "#         ])\n",
    "\n",
    "#         # Create the chain\n",
    "#         self.chain = self.prompt | self.llm | StrOutputParser()\n",
    "\n",
    "#     def __call__(self, text):\n",
    "#         return self.chain.invoke({\"user_input\": text})\n",
    "\n",
    "\n",
    "# # ---- USAGE ----\n",
    "# chat = OllamaChatChain(system_prompt=\"You are a witty comedian.\")\n",
    "\n",
    "# print(chat(\"Tell me a joke about computers.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8469905-32e6-4418-9c10-226e8f1f2e26",
   "metadata": {},
   "source": [
    "# LiteLLM\n",
    "\n",
    "LiteLLM is an open-source Python/JS library that provides a unified API for calling many different large-language-model providers using an OpenAI-style interface.\n",
    "\n",
    "Think of it as a ‚Äúmodel router‚Äù or compatibility layer: you can switch between OpenAI, Anthropic, Google Gemini, Azure, local models (like Ollama), and others without changing your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c33f9632-0e00-4dd7-b2b3-a9a898425dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install liteLLM if not installed\n",
    "# !pip install litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88a7e374-01a4-45ad-ab6f-0792ce5fd4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion\n",
    "\n",
    "response_litellm = completion(\n",
    "            model=\"ollama/gemma3\",\n",
    "            messages = tell_a_joke,\n",
    "            api_base=\"http://localhost:11434\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fbde4e3-cc7b-4cb7-9c0a-6b9da00c5dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelResponse(id='chatcmpl-118c30b2-080d-44d0-ae53-1f7b4bc49ef2', created=1763528588, model='ollama/gemma3', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Okay, here‚Äôs a joke for a student on the journey to becoming an LLM Engineering expert:\\n\\nWhy did the LLM cross the road? \\n\\n‚Ä¶ To optimize its attention mechanism and generate a more nuanced response! \\n\\n---\\n\\nWould you like to hear another one, or maybe a joke related to a specific area of LLM engineering (like prompt engineering or fine-tuning)?', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None, reasoning_content=None))], usage=Usage(completion_tokens=81, prompt_tokens=31, total_tokens=112, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "print(response_litellm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3ba5e9f-852e-46fe-b402-bb80981ab799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, here‚Äôs a joke for a student on the journey to becoming an LLM Engineering expert:\n",
       "\n",
       "Why did the LLM cross the road? \n",
       "\n",
       "‚Ä¶ To optimize its attention mechanism and generate a more nuanced response! \n",
       "\n",
       "---\n",
       "\n",
       "Would you like to hear another one, or maybe a joke related to a specific area of LLM engineering (like prompt engineering or fine-tuning)?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reply = response_litellm.choices[0].message.content\n",
    "display(Markdown(reply))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45e2df8a-fe28-46b6-8ebc-5231899111fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 31\n",
      "Output tokens: 81\n",
      "Total tokens: 112\n",
      "Total cost: 0.0000 cents\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input tokens: {response_litellm.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response_litellm.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response_litellm.usage.total_tokens}\")\n",
    "print(f\"Total cost: {response_litellm._hidden_params[\"response_cost\"]*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e150732-a3e2-41bf-a5d1-6e3aadfa82eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'custom_llm_provider': 'ollama',\n",
       " 'region_name': None,\n",
       " 'optional_params': {},\n",
       " 'litellm_call_id': '84494e4e-1557-4736-a086-7dc3b3038565',\n",
       " 'api_base': 'http://localhost:11434',\n",
       " 'model_id': None,\n",
       " 'response_cost': 0.0,\n",
       " 'additional_headers': {},\n",
       " 'litellm_model_name': 'ollama/gemma3',\n",
       " '_response_ms': 4129.165999999999}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_litellm._hidden_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72f96ee-2fe2-42ee-b7c2-ec0c81b30827",
   "metadata": {},
   "source": [
    "# Now - let's use LiteLLM to illustrate a Pro-feature: Prompt caching\n",
    "\n",
    "**Prompt caching** is a technique used to speed up LLM applications and reduce cost by reusing (or partially reusing) results from previous model calls instead of recomputing them every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9743a280-e7b8-42fa-b843-98f056059cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speak, man.\n",
      "  Laer. Where is my father?\n",
      "  King. Dead.\n",
      "  Queen. But not by him!\n",
      "  King. Let him deman\n"
     ]
    }
   ],
   "source": [
    "with open(\"/home/alexender/Downloads/hamlet.txt\", 'r', encoding='utf-8')as f:\n",
    "    hamlet = f.read()\n",
    "\n",
    "loc = hamlet.find(\"Speak, man\")\n",
    "print(hamlet[loc:loc+100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c71e186-23bf-4c8e-b11e-4cd76cd863ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The reply to Laertes‚Äôs question, ‚ÄúWhere is my father?‚Äù in Hamlet is:\n",
       "\n",
       "**‚ÄúHe is gone to France.‚Äù**\n",
       "\n",
       "This is delivered by Hamlet, and it's a deliberately vague and misleading response intended to frustrate and enrage Laertes, furthering the play's central themes of deception and suspicion."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 32\n",
      "Output tokens: 68\n",
      "Total tokens: 100\n",
      "Total cost: 0.0000 cents\n"
     ]
    }
   ],
   "source": [
    "question = [{\"role\": \"user\", \"content\": \"In Hamlet, when Laertes asks 'Where is my father?' what is the reply?\"}]\n",
    "\n",
    "response = completion(\n",
    "    model=\"ollama/gemma3\",\n",
    "    messages = question,\n",
    "    api_base=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "reply = response.choices[0].message.content\n",
    "display(Markdown(reply))\n",
    "\n",
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea774bbb-20fd-448a-a1a8-3c4dc2db5079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "In *Hamlet*, when Laertes asks ‚ÄúWhere is my father?‚Äù Horatio replies:\n",
       "\n",
       "‚Äú**He is dead, doubled, like a Valkyrie.**‚Äù \n",
       "\n",
       "This is a key moment in the play, revealing the truth about Hamlet's father‚Äôs death and setting the stage for the tragic events to come."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Some times it can answer wrongly so we are adding the context\n",
    "\n",
    "question[0][\"content\"] = \"\\n\\nFor context, here is the entire text of Hamlet:\\n\\n\"+hamlet+question[0][\"content\"]\n",
    "\n",
    "response = completion(\n",
    "    model=\"ollama/gemma3\",\n",
    "    messages = question,\n",
    "    api_base=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "reply = response.choices[0].message.content\n",
    "display(Markdown(reply))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4e7f5d2-b4d8-4517-a900-066e7c4dd517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 4096\n",
      "Output tokens: 68\n",
      "Total tokens: 4164\n",
      "Total cost: 0.0000 cents\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e26d98-d228-4485-8940-2d9f4bfed9ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
