{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "731019bc-0ace-4471-a932-cc2ee344ced2",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "A **model** is a machine learning algorithm that has been trained to perform a specific NLP task, like text generation, classification, or translation. Hugging Face provides access to many pre-trained models from state-of-the-art architectures like:\n",
    "\n",
    "* **BERT (Bidirectional Encoder Representations from Transformers):** Primarily used for understanding the context of text (e.g., classification, NER).\n",
    "\n",
    "* **GPT (Generative Pretrained Transformer):** Primarily used for generating text or continuation of text.\n",
    "\n",
    "* **T5 (Text-to-Text Transfer Transformer):** A versatile model that treats every task as a text generation problem (e.g., summarization, translation).\n",
    "\n",
    "* **DistilBERT:** A smaller, faster version of BERT, designed to be more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c65179c-7249-4a00-9f2e-7b236b6f7c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
    "import torch\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91147e1e-4c3d-4f2f-aec6-7bbcc643116f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading HF Token\n",
    "\n",
    "hf_token = os.getenv(\"HUGGING_FACE_WRITE_TOKEN\")\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3cbcbc6-0fe6-4b04-b572-30f9a2d31e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruct models\n",
    "\n",
    "LLAMA = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "PHI = \"microsoft/Phi-4-mini-instruct\"\n",
    "GEMMA = \"google/gemma-3-270m-it\"\n",
    "QWEN = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "\n",
    "# Reasoning model\n",
    "\n",
    "DEEPSEEK = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba775592-cddd-47bd-a5b6-464b54a4ebce",
   "metadata": {},
   "source": [
    "# Quantization in Large Language Models (LLMs)\n",
    "\n",
    "## 1. Definition\n",
    "- Quantization reduces the numerical precision of model parameters\n",
    "- Converts high-precision values (FP32 / FP16) to lower precision (INT8, INT4, etc.)\n",
    "- Primarily used for inference optimization\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Purpose of Quantization\n",
    "- Reduce model size\n",
    "- Speed up inference\n",
    "- Lower memory and compute requirements\n",
    "- Enable deployment on edge and low-resource devices\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Benefits\n",
    "- Smaller disk and memory footprint\n",
    "- Faster inference latency\n",
    "- Lower power consumption\n",
    "- Cost-effective deployment at scale\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Trade-offs\n",
    "- Slight degradation in accuracy\n",
    "- Aggressive quantization may affect:\n",
    "  - Reasoning quality\n",
    "  - Numerical stability\n",
    "  - Long-context performance\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Common Precision Levels\n",
    "- FP32: highest accuracy, very large size\n",
    "- FP16 / BF16: near-FP32 accuracy, smaller size\n",
    "- INT8: good balance of size and accuracy\n",
    "- INT4: very small, faster inference\n",
    "- INT2: experimental, significant accuracy loss\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Types of Quantization\n",
    "\n",
    "### 6.1 Post-Training Quantization (PTQ)\n",
    "- Applied after model training\n",
    "- No retraining required\n",
    "- Most widely used in practice\n",
    "\n",
    "### 6.2 Quantization-Aware Training (QAT)\n",
    "- Model trained with quantization simulation\n",
    "- Higher accuracy than PTQ\n",
    "- Computationally expensive\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Popular Quantization Methods\n",
    "- GPTQ: layer-wise optimization, good 4-bit quality\n",
    "- AWQ: activation-aware, better accuracy at low bit-widths\n",
    "- GGUF: optimized for CPU inference (llama.cpp)\n",
    "\n",
    "---\n",
    "\n",
    "## 8. What Gets Quantized\n",
    "- Model weights (always)\n",
    "- Activations (sometimes)\n",
    "- KV cache (important for long context efficiency)\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Memory Impact Example\n",
    "- 7B model FP16 ≈ 14 GB\n",
    "- 7B model INT8 ≈ 7 GB\n",
    "- 7B model INT4 ≈ 3.5 GB\n",
    "\n",
    "---\n",
    "\n",
    "## 10. When to Use Quantization\n",
    "- Running models locally\n",
    "- Large-scale inference deployment\n",
    "- Memory- or latency-constrained environments\n",
    "\n",
    "---\n",
    "\n",
    "## 11. When to Avoid Heavy Quantization\n",
    "- High-precision reasoning tasks\n",
    "- Training or fine-tuning scenarios\n",
    "- Accuracy-critical applications\n",
    "\n",
    "---\n",
    "\n",
    "## 12. Key Takeaway\n",
    "- Quantization trades minimal accuracy for major gains in efficiency\n",
    "- Essential for practical LLM deployment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41988ab8-9b65-4119-a7ea-923438a0d17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell a joke for a room of Data Scientists\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ba62461-8522-4054-b641-d8f9d46c758c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization Config - this allows us to load the model into memory and use less memory\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # Loads model weights in 4-bit precision, drastically reducing memory usage at the cost of a small accuracy drop.\n",
    "    bnb_4bit_use_double_quant=True, # Applies double quantization to further compress 4-bit weights, saving additional memory with minimal quality impact.\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # Performs computations in bfloat16 for improved numerical stability while keeping weights in 4-bit storage.\n",
    "    bnb_4bit_quant_type=\"nf4\" # Uses NF4 quantization optimized for normally distributed LLM weights, giving better accuracy than standard INT4 at the same size.\n",
    ")\n",
    "# NF4 is a 4-bit data type designed specifically for LLM weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7166066d-6e13-464b-aa2d-48fe5bc64480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# LLaMA models do not have a native padding token, so we set pad_token = eos_token to make batching and tensor operations work without breaking the model.\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "\n",
    "# return_tensor=pt will return tensor after the calculations as pytorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "884f4ada-6144-4b8f-9a84-5ef5f64bb001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
       "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
       "            220,   1544,   3799,    220,   2366,     20,    271, 128009, 128006,\n",
       "            882, 128007,    271,  41551,    264,  22380,    369,    264,   3130,\n",
       "            315,   2956,  57116, 128009]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b220acf-3485-4099-894b-a794e9e9e728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-27 18:06:03,406] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(LLAMA, device_map=\"auto\", quantization_config=quant_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b18af95b-b172-456b-b30f-01c19cf2c01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 1,012.0 MB\n"
     ]
    }
   ],
   "source": [
    "memory = model.get_memory_footprint() / 1e6\n",
    "print(f\"Memory footprint: {memory:,.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8247a61-f660-484b-9e2e-e23426866400",
   "metadata": {},
   "source": [
    "## Looking under the hood at the Transformer model\n",
    "\n",
    "The next cell prints the HuggingFace `model` object for Llama.\n",
    "\n",
    "This model object is a Neural Network, implemented with the Python framework PyTorch. The Neural Network uses the architecture invented by Google scientists in 2017: the Transformer architecture.\n",
    "\n",
    "While we're not going to go deep into the theory, this is an opportunity to get some intuition for what the Transformer actually is.\n",
    "\n",
    "If you're completely new to Neural Networks, check out [YouTube intro playlist](https://www.youtube.com/playlist?list=PLWHe-9GP9SMMdl6SLaovUQF2abiLGbMjs) for the foundations.\n",
    "\n",
    "Now take a look at the layers of the Neural Network that get printed in the next cell. Look out for this:\n",
    "\n",
    "- It consists of layers\n",
    "- There's something called \"embedding\" - this takes tokens and turns them into 4,096 dimensional vectors. We'll learn more about this in Week upcoming.\n",
    "- There are then 16 sets of groups of layers (32 for Llama 3.1) called \"Decoder layers\". Each Decoder layer contains three types of layer: (a) self-attention layers (b) multi-layer perceptron (MLP) layers (c) batch norm layers.\n",
    "- There is an LM Head layer at the end; this produces the output\n",
    "\n",
    "Notice the mention that the model has been quantized to 4 bits.\n",
    "\n",
    "It's not required to go any deeper into the theory at this point, but if you'd like to, I've asked our mutual friend to take this printout and make a tutorial to walk through each layer. This also looks at the dimensions at each point. If you're interested, work through this tutorial after running the next cell:\n",
    "\n",
    "https://chatgpt.com/canvas/shared/680cbea6de688191a20f350a2293c76b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0c8f51c-17c8-4e37-b5c6-c9dd4741496f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute this cell and look at what gets printed; investigate the layers\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406e60a3-79e8-473a-9e7f-731e00bfa05f",
   "metadata": {},
   "source": [
    "### And if you want to go even deeper into Transformers\n",
    "\n",
    "In addition to looking at each of the layers in the model, you can actually look at the HuggingFace code that implements Llama using PyTorch.\n",
    "\n",
    "Here is the HuggingFace Transformers repo:  \n",
    "https://github.com/huggingface/transformers\n",
    "\n",
    "And within this, here is the code for Llama 4:  \n",
    "https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama4/modeling_llama4.py\n",
    "\n",
    "It's a fascinating rabbit hole if you're interested!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "958ee3e8-ea66-4883-a3c2-54fe1f3b42fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
       "            25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
       "           220,   1544,   3799,    220,   2366,     20,    271, 128009, 128006,\n",
       "           882, 128007,    271,  41551,    264,  22380,    369,    264,   3130,\n",
       "           315,   2956,  57116, 128009, 128006,  78191, 128007,    271,   8586,\n",
       "           596,    264,  22380,    369,    264,   3130,    315,    828,  14248,\n",
       "          1473,  10445,   1550,    279,    828,  28568,   1464,    709,    449,\n",
       "           813,  23601,   1980,  18433,    568,   4934,    311,  24564,    872,\n",
       "          5133,    323,   1505,    264,    810,  11297,   3717,   2268,     40,\n",
       "          3987,    420,  22380,  12716,    264,  15648,    311,    872,  12580,\n",
       "           323,   8779,   1124,  12234,   1306,    264,   1317,   1938,    315,\n",
       "          3318,    449,   5219,      0, 128009], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OK, with that, now let's run the model!\n",
    "\n",
    "outputs = model.generate(inputs, max_new_tokens=80)\n",
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "713d9d28-bdae-4d7b-92da-2b9efe382138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 Dec 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell a joke for a room of Data Scientists<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nHere's a joke for a room of data scientists:\\n\\nWhy did the data scientist break up with his girlfriend?\\n\\nBecause he wanted to analyze their relationship and find a more efficient connection!\\n\\nI hope this joke brings a smile to their faces and helps them relax after a long day of working with numbers!<|eot_id|>\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Well that doesn't make much sense!\n",
    "# How about this..\n",
    "\n",
    "tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e771796-cef4-4c50-8185-9850d5e78785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up memory like previous notebooks\n",
    "\n",
    "del model, inputs, tokenizer, outputs\n",
    "gc.collect()\n",
    "torch.cuda.ipc_collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a325184b-9f65-4b4d-b6c3-ac446d6c2b16",
   "metadata": {},
   "source": [
    "## A couple of quick notes on the next block of code:\n",
    "\n",
    "I'm using a HuggingFace utility called TextStreamer so that results stream back.\n",
    "To stream results, we simply replace:  \n",
    "`outputs = model.generate(inputs, max_new_tokens=80)`  \n",
    "With:  \n",
    "`streamer = TextStreamer(tokenizer)`  \n",
    "`outputs = model.generate(inputs, max_new_tokens=80, streamer=streamer)`\n",
    "\n",
    "Also I've added the argument `add_generation_prompt=True` to my call to create the Chat template. This ensures that Phi generates a response to the question, instead of just predicting how the user prompt continues. Try experimenting with setting this to False to see what happens. You can read about this argument here:\n",
    "\n",
    "https://huggingface.co/docs/transformers/main/en/chat_templating#what-are-generation-prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b946abe6-14d1-4541-b679-0cb4ae4130a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping everything in a function - and adding Streaming and generation prompts\n",
    "\n",
    "def generate(model, messages, quant=True, max_new_tokens=80):\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "  input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
    "  attention_mask = torch.ones_like(input_ids, dtype=torch.long, device=\"cuda\")\n",
    "  streamer = TextStreamer(tokenizer)\n",
    "  if quant:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model, quantization_config=quant_config).to(\"cuda\")\n",
    "  else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model).to(\"cuda\")\n",
    "  outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=max_new_tokens, streamer=streamer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcd3b44-1ca3-4581-8b93-840342a4eb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(PHI, messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82489857-5ae9-445a-8f16-268454198f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell a light-hearted joke for a room of Data Scientists\"}\n",
    "  ]\n",
    "generate(GEMMA, messages, quant=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238b508e-9172-4906-80bf-14b87eb8a431",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(DEEPSEEK, messages, quant=False, max_new_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30e96b2-8714-4426-bc77-90b34ed6668d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
