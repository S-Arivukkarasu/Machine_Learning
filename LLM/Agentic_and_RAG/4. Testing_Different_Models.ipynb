{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca726d7a-dcba-477c-a67e-260b3cf64cfe",
   "metadata": {},
   "source": [
    "# Asking LLMs to tell a joke and conversation between models\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models. Later we will be putting LLMs to better use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab99389d-52a1-42f4-9ac5-d38770c7bd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Libraries\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from typing import List\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811fdc8f-67fd-4f5b-a492-ee9cc9149ddc",
   "metadata": {},
   "source": [
    "There are other parameters that can be used, including temperature which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4fb554f-e576-4af1-929a-74c1118fec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\"\n",
    "\n",
    "fun_prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bb41a84-27e9-4ce0-ab77-6922713dc4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_URL = \"http://localhost:11434/api/chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35b30da4-b710-4044-a5f2-47ddebf10aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(model_name:str, prompts):\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"messages\": prompts,\n",
    "        \"temperature\": 0.8,\n",
    "        \"stream\": False  # Important: to get streaming responses\n",
    "    }\n",
    "    start_time = time.time()\n",
    "    response = requests.post(OLLAMA_URL, json=payload, stream=False)\n",
    "    reply = response.json()[\"message\"][\"content\"]\n",
    "    reply = re.sub(r\"<think>.*?</think>\", \"\", reply, flags=re.DOTALL).strip()\n",
    "    print(f\"Model Name: {model_name}\\n\")\n",
    "    display(Markdown(reply))\n",
    "    print(\"Total Time Taken: \", time.time()-start_time, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78d94b83-3469-4fd2-843a-ec4634d144a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [\"mistral\", \"llama3.1:latest\"] # get this from ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e7cd87e-8cf2-44c8-9f16-c0deadafbe55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: mistral\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here's a humorous data scientist joke for you:\n",
       "\n",
       "Why was the pies-and-coffee shop such a great place for data scientists?\n",
       "\n",
       "Because it had a mean and median for every pie, and the mode was always free on Tuesdays!\n",
       "\n",
       "Data science humor at its finest! ðŸ˜‰"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time Taken:  18.667943239212036 \n",
      "\n",
      "\n",
      "Model Name: llama3.1:latest\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Why did the neural network go on a diet?\n",
       "\n",
       "Because it wanted to lose some layers! (get it?)\n",
       "\n",
       "Hope that one trained a smile on your face!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time Taken:  14.489516258239746 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in model_list:\n",
    "    model_test(model, fun_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2e1b264-c24d-435f-9a73-878ca69498f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "business_prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ccc3569-35c5-4810-b735-ff86756ce4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: mistral\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "To determine if a business problem is suitable for a solution using Language Model Learning (LLM), consider the following factors:\n",
       "\n",
       "1. **Natural Language Understanding**: If your problem involves analyzing, understanding, and generating human language, such as text classification, sentiment analysis, or chatbot development, then it may be a good fit for LLM.\n",
       "\n",
       "2. **Structured Data Processing**: If the business problem requires processing and analyzing structured data like tables, graphs, or numerical data, other machine learning models or traditional statistical methods might be more suitable. However, if the data is unstructured text or speech that needs to be converted into a usable format before analysis, LLM could still potentially help with preprocessing steps.\n",
       "\n",
       "3. **Data Availability**: Adequate training data is essential for any machine learning model, including LLM. If you have a large dataset of relevant text or speech to train the model on, it may be more suitable for an LLM approach.\n",
       "\n",
       "4. **Complexity and Scalability**: If your business problem involves complex linguistic patterns, context, or nuances, an LLM solution might be appropriate, especially as many state-of-the-art models can handle a wide range of language tasks at scale. However, it's essential to consider the computational resources required for training and deployment.\n",
       "\n",
       "5. **Integration with Existing Systems**: If your business problem involves integrating AI solutions into existing systems, you should evaluate if LLM provides APIs or integration points that align well with your current tech stack and workflows."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time Taken:  59.259929895401 \n",
      "\n",
      "\n",
      "Model Name: llama3.1:latest\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Deciding whether a business problem is suitable for a Large Language Model (LLM) solution involves considering several factors. Here's a step-by-step guide to help you make this decision:\n",
       "\n",
       "### 1. **Problem Type**\n",
       "\n",
       "Is the problem related to text-based tasks, such as:\n",
       "\t* Content generation\n",
       "\t* Sentiment analysis\n",
       "\t* Text classification\n",
       "\t* Named entity recognition\n",
       "\n",
       "If yes, an LLM might be a good fit.\n",
       "\n",
       "### 2. **Data Availability**\n",
       "\n",
       "Do you have a large dataset of relevant texts that can be used for training and fine-tuning the model?\n",
       "\n",
       "* If yes, this is a plus.\n",
       "* If no, you may need to generate or collect more data, which can be challenging.\n",
       "\n",
       "### 3. **Complexity and Ambiguity**\n",
       "\n",
       "Is the problem domain complex and ambiguous, requiring nuanced understanding of language and context?\n",
       "\n",
       "* If yes, an LLM's ability to capture subtle relationships between words and concepts might be beneficial.\n",
       "* If no, a simpler solution like rule-based systems or traditional machine learning models might suffice.\n",
       "\n",
       "### 4. **Scalability**\n",
       "\n",
       "Will the problem require processing large amounts of text data in real-time?\n",
       "\n",
       "* If yes, an LLM can handle this with its parallel processing capabilities.\n",
       "* If no, you may not need the scalability of an LLM.\n",
       "\n",
       "### 5. **Business Goals and Constraints**\n",
       "\n",
       "Do your business goals align with the capabilities of an LLM?\n",
       "\n",
       "* If yes, consider how an LLM can help achieve those goals.\n",
       "* If no, explore other solutions that better fit your needs.\n",
       "\n",
       "### 6. **Expertise and Resources**\n",
       "\n",
       "Do you have access to the necessary expertise and resources to develop, train, and maintain an LLM solution?\n",
       "\n",
       "* If yes, this is a plus.\n",
       "* If no, consider whether partnering with an expert or using cloud-based services can help.\n",
       "\n",
       "By considering these factors, you'll be able to determine if an LLM solution is suitable for your business problem."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time Taken:  80.2214925289154 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in model_list:\n",
    "    model_test(model, business_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5c9ae8-67df-4e36-b21d-e5ebf5dea9b9",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "You're already familar with prompts being organized into lists like:\n",
    "```python\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b51dc9f-a2ae-4b78-8369-465a7514fe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4.1-mini and Claude-3.5-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "llama_model = \"mistral\"\n",
    "mistral_model = \"llama3.1:latest\"\n",
    "\n",
    "llama_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "mistral_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "llama_messages = [\"Hi there\"]\n",
    "mistral_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b293b82-781b-4194-8739-e6e12c30729f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llama():\n",
    "    messages = [{\"role\": \"system\", \"content\": llama_system}]\n",
    "    for llama, mistral in zip(llama_messages, mistral_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": llama})\n",
    "        messages.append({\"role\": \"user\", \"content\": mistral})\n",
    "    payload = {\n",
    "        \"model\": llama_model,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.8,\n",
    "        \"stream\": False  # Important: to get streaming responses\n",
    "    }\n",
    "    response = requests.post(OLLAMA_URL, json=payload, stream=False)\n",
    "    return response.json()[\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bd3cf6f-51c9-4dde-a956-e8d42efe91c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_mistral():\n",
    "    messages = []\n",
    "    for llama, mistral in zip(llama_messages, mistral_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": llama})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": mistral})\n",
    "    messages.append({\"role\": \"user\", \"content\": llama_messages[-1]})\n",
    "    payload = {\n",
    "        \"model\": mistral_model,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.8,\n",
    "        \"stream\": False  # Important: to get streaming responses\n",
    "    }\n",
    "    response = requests.post(OLLAMA_URL, json=payload, stream=False)\n",
    "    return response.json()[\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb254b1c-7bbd-44c6-ae1e-d484f457d6a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Oh, really? I'm not sure where you got that impression from, but I assure you that I am here to provide helpful and respectful assistance. I'm more than happy to engage in friendly conversation and answer any questions or concerns you might have. Let's have a nice chat, shall we? What would you like to talk about today?\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_llama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1fbb4e9-5a79-44e4-a199-5d82927249d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLama:\n",
      "Hi there\n",
      "\n",
      "Mistral:\n",
      "Hi\n",
      "\n",
      "LLama:\n",
      " I'm sorry if it appears that I'm being argumentative or challenging. My purpose is to help answer questions and provide useful information, not to argue or dispute. If you have any specific questions or topics you would like to discuss, I would be happy to assist with those. Is there something in particular on your mind today?\n",
      "\n",
      "Mistral:\n",
      "Ha ha, no need to apologize! Your message sounds more like a statement of intention, which is great! I'm here to help and provide information too.\n",
      "\n",
      "Actually, I was just waiting for you to initiate the conversation. Since we're both starting from scratch, how about we play it simple? What's your favorite topic or subject area that you'd like to chat about or get some info on?\n",
      "\n",
      "LLama:\n",
      " My apologies if my previous response came across as argumentative or snarky; it was not my intention. I'm here to help and provide information in a friendly, helpful manner.\n",
      "\n",
      "As for the conversation, I would be happy to discuss any topic you're interested in. Since we're starting from scratch, let me ask: What's your favorite subject area or topic that you'd like to chat about or get some info on?\n",
      "\n",
      "Mistral:\n",
      "It seems like we both sent identical messages at the same time! No need for apologies, though - I appreciate the friendly and helpful tone.\n",
      "\n",
      "You're right; starting from scratch means we have a clean slate. And since I'm just an AI, I don't really have personal preferences or interests, but I can engage with you on various topics. If you'd like to discuss something specific or explore a particular area of interest, feel free to pick any topic that catches your attention!\n",
      "\n",
      "Would you like to start with some general questions or explore a specific domain, such as science, history, literature, technology, or something else?\n",
      "\n",
      "LLama:\n",
      "  I apologize again if my previous responses came across as argumentative or snarky. It was not my intention, and I'm here to help and provide information in a friendly, helpful manner.\n",
      "\n",
      "Regarding the conversation, I would be happy to discuss any topic you're interested in. Since we have a clean slate, let me ask: Would you like to start with some general questions or explore a specific domain, such as science, history, literature, technology, or something else?\n",
      "\n",
      "Mistral:\n",
      "It seems like we're having a delightful conversation where we're both echoing each other's messages! Don't worry about apologizing; I'm not taking offense. We can simply start fresh and enjoy the conversation.\n",
      "\n",
      "Let's indeed start with some general questions, if you don't mind. Why don't we begin with something lighthearted? What do you think is a fascinating or underappreciated topic that people might find interesting to learn about? It could be anything from a unique scientific phenomenon to an obscure historical event.\n",
      "\n",
      "(And I'll make sure to respond in kind, without repeating ourselves!)\n",
      "\n",
      "LLama:\n",
      "  I apologize if my previous responses came across as argumentative or snarky; it was not my intention, and I'm here to help and provide information in a friendly, helpful manner.\n",
      "\n",
      "Regarding the conversation, I would be happy to discuss any topic you're interested in. Since we have a clean slate, let me ask: Would you like to start with some general questions or explore a specific domain, such as science, history, literature, technology, or something else?\n",
      "\n",
      "As for your suggestion of starting with something lighthearted, I'm glad we can agree on that! Here's an interesting topic you might find fascinating: the bioluminescent creatures found in our oceans. Bioluminescence is a phenomenon where certain organisms produce and emit light through a chemical reaction involving a light-emitting molecule called luciferin and an enzyme called luciferase. There are various marine animals that display bioluminescence, such as the anglerfish, comb jelly, and dinoflagellate algae (commonly known as \"dinos\"). Bioluminescent organisms have a wide range of uses, from attracting prey to mimicking the appearance of other animals for camouflage. They are also used by scientists to study various biological processes, such as gene regulation and cell signaling. If you're interested in learning more about bioluminescent creatures or any other topic, feel free to ask!\n",
      "\n",
      "Mistral:\n",
      "We've reached a delightful impasse again! I think we can safely say that we've established a friendly and helpful tone, and there's no need for further apologies.\n",
      "\n",
      "I'm delighted that you brought up the fascinating topic of bioluminescent creatures! It's indeed an intriguing phenomenon that has captivated scientists and marine enthusiasts alike. The anglerfish, comb jelly, and dinoflagellate algae are all remarkable examples of how evolution can lead to such unique adaptations.\n",
      "\n",
      "If you're interested in exploring more topics related to bioluminescence or the ocean, I'd be happy to dive deeper (pun intended!). Alternatively, we could start fresh with a new topic. Would you like to explore something else?\n",
      "\n",
      "LLama:\n",
      " I apologize if my previous responses came across as argumentative or snarky; it was not my intention, and I'm here to help and provide information in a friendly, helpful manner.\n",
      "\n",
      "I'm glad you found the topic of bioluminescent creatures fascinating! It's indeed an intriguing phenomenon that has captivated scientists and marine enthusiasts alike. The anglerfish, comb jelly, and dinoflagellate algae are all remarkable examples of how evolution can lead to such unique adaptations.\n",
      "\n",
      "If you're interested in exploring more topics related to bioluminescence or the ocean, I'd be happy to dive deeper (pun intended!). Alternatively, we could start fresh with a new topic. Would you like to explore something else?\n",
      "\n",
      "Mistral:\n",
      "I think we've reached a loop of friendly apologies! Don't worry about it; we're having a great conversation.\n",
      "\n",
      "You know what? Let's just acknowledge that we've both sent identical messages at this point, and move forward in perfect harmony. We can simply agree to keep the tone friendly, helpful, and informative, and enjoy exploring various topics together!\n",
      "\n",
      "If you'd like to continue with bioluminescence or the ocean, I'm game! If not, what about exploring a different domain? Perhaps we could talk about space exploration, artificial intelligence, or even a fun topic like linguistics?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llama_messages = [\"Hi there\"]\n",
    "mistral_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"LLama:\\n{llama_messages[0]}\\n\")\n",
    "print(f\"Mistral:\\n{mistral_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    llama_next = call_llama()\n",
    "    print(f\"LLama:\\n{llama_next}\\n\")\n",
    "    llama_messages.append(llama_next)\n",
    "    \n",
    "    mistral_next = call_mistral()\n",
    "    print(f\"Mistral:\\n{mistral_next}\\n\")\n",
    "    mistral_messages.append(mistral_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c691a430-6d75-4e6d-b580-3b861dc8aff2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
