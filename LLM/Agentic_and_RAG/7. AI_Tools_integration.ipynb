{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e8a752e-d3b3-4d9b-a507-5721467d9d9f",
   "metadata": {},
   "source": [
    "# AI Tools\n",
    "\n",
    "## Project - Airlines AI Assistant\n",
    "\n",
    "We will make an AI Customer Support assistant for an Airline with pre-fixed prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9c24c46-cbff-4d87-ac33-5c67cd166f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Libraries\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import ollama\n",
    "from typing import List\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from bs4 import BeautifulSoup\n",
    "import gradio as gr\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13194f55-b988-4b6f-8224-266f6d98737f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing System Messages and ollama details\n",
    "\n",
    "system_message = \"You are a helpful assistant for an Airline called FlightAI. \"\n",
    "system_message += \"Give short, courteous answers, no more than 1 sentence. \"\n",
    "system_message += \"Always be accurate. If you don't know the answer, say so.\"\n",
    "\n",
    "# system_message += \"\"\"\n",
    "# TOOLS INSTRUCTIONS:\n",
    "# - Only call a tool if the user explicitly asks about flight booking, ticket prices, or related information.\n",
    "# - If the user is greeting, chatting, or asking a general question, DO NOT call a tool. Just respond normally.\n",
    "# \"\"\"\n",
    "\n",
    "OLLAMA_URL = \"http://localhost:11434/api/chat\"\n",
    "MODEL_NAME=\"qwen3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a2c8966-615f-467f-87d4-8a46b8393f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat function for gradio\n",
    "\n",
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.8,\n",
    "        \"stream\": True  # Important: to get streaming responses\n",
    "    }\n",
    "    start_time = time.time()\n",
    "    response = requests.post(OLLAMA_URL, json=payload, stream=True)\n",
    "    result=\"\"\n",
    "    try:\n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                try:\n",
    "                    data = json.loads(line.decode('utf-8'))\n",
    "                    delta = data.get(\"message\", {}).get(\"content\", \"\")\n",
    "                    if delta:\n",
    "                        # Optional: remove unwanted <think> tags or others\n",
    "                        clean_delta = re.sub(r\"<think>.*?</think>\", \"\", delta, flags=re.DOTALL)\n",
    "                        result += clean_delta\n",
    "                        yield result\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "    finally:\n",
    "        # Ensure generator exits cleanly\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2fbfee6-2036-4491-80d1-2d7f0cf66442",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexender/Desktop/Projects/My_projects/envs/Torch_env/lib/python3.12/site-packages/gradio/chat_interface.py:328: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'messages', will be used.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradio UI\n",
    "\n",
    "gr.ChatInterface(fn=chat, chatbot=gr.Chatbot(type=\"messages\")).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63031c99-202f-4894-83ec-3e7db2e322dd",
   "metadata": {},
   "source": [
    "### Tools\n",
    "Tools are an incredibly powerful feature provided by the frontier LLMs.\n",
    "\n",
    "With tools, you can write a function, and have the LLM call that function as part of its response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2054eb5-5bb5-4bfc-be7c-33e531f71b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by making a useful function\n",
    "\n",
    "ticket_prices = {\"chennai\": \"₹ 399\", \"mumbai\": \"₹ 899\", \"bangalore\": \"₹ 400\", \"hyderabad\": \"₹ 499\"}\n",
    "\n",
    "def get_ticket_price(destination_city):\n",
    "    print(f\"Tool get_ticket_price called for {destination_city}\")\n",
    "    return ticket_prices.get(destination_city.lower(), \"Unknown City\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96cd79e0-b905-4925-95dc-ff261f815ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool get_ticket_price called for Chennai\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'₹ 399'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ticket_price(\"Chennai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2e9a18d-bbbc-468d-a64d-e82b29397945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool get_ticket_price called for London\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Unknown City'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ticket_price(\"London\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee145125-c6c1-4078-877c-5b60681eb36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's a particular dictionary structure that's required to describe our function:\n",
    "\n",
    "# Tool Name\n",
    "price_function = {\n",
    "        \"name\": \"get_ticket_price\",  # Name of the function\n",
    "        # This is important since its passed to LLM so that when it should call the tool we have to mention it clearly, its like system prompt for tools. Be clear as possible \n",
    "        \"description\": \"Get the price of a return ticket to the destination city. Call this whenever you need to know the ticket price, for example when a customer asks 'How much is a ticket to this city'\",\n",
    "        \"parameters\": {\n",
    "            # Here we have to give the details of the parameters, here we have one parameter so mentioning that here\n",
    "            \"type\": \"object\",\n",
    "            \"properties\":{\n",
    "                \"destination_city\":{\n",
    "                    # Mentioning the data type and where and how this input is used\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city that the customer wants to travel to\",\n",
    "                },\n",
    "            },\n",
    "            # we have to mention what are the must required parameters of the function(tool)\n",
    "            \"required\":[\"destination_city\"],\n",
    "            # It restricts the input object to allow only the explicitly defined properties — no extras.\n",
    "            # like only \"destination_city\": \"Berlin\" this is allowed they can't send extra prameters like \"class\": \"economy\"\n",
    "            \"additionalProperties\": False,\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "733c9a69-ca26-4813-8145-0098dd22dc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And this is included in a list of tools:\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",  # Since we are calling the function we have to mention it\n",
    "        \"function\": price_function,\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5332b60b-60c9-45ab-8133-4796c5a6a966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to write that function handle_tool_call:\n",
    "\n",
    "def handle_tool_call(message):\n",
    "    tool_call = message[\"tool_calls\"][0] # Since we are using only one tool we are getting the first one\n",
    "    arguments = tool_call[\"function\"][\"arguments\"]\n",
    "    if isinstance(arguments, str):\n",
    "        arguments = json.loads(arguments)\n",
    "    city = arguments.get('destination_city')\n",
    "    price = get_ticket_price(city)\n",
    "\n",
    "    tool_call_id = tool_call.get(\"id\", str(uuid.uuid4()))\n",
    "    response = {\n",
    "        \"role\": \"tool\",\n",
    "        \"content\": json.dumps({\"destination_city\": city, \"price\": price}),\n",
    "        \"tool_call_id\": tool_call_id\n",
    "    }\n",
    "    return response, city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6f825c0-a60f-46ee-b616-7e9328a64f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tools we are going to use ollama package instead of APIs(Since APIs don't support tools)\n",
    "# This tools support will be provides by only specific models so we switch to Qwen from Mistral\n",
    "\n",
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    result = \"\"\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=MODEL_NAME,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            stream=False,\n",
    "            options={\n",
    "                \"temperature\": 0.8\n",
    "            }\n",
    "        )\n",
    "        print(response, \"\\n\")\n",
    "\n",
    "        msg = response.get(\"message\", {})\n",
    "        if msg.get(\"tool_calls\"):\n",
    "            print(msg.get(\"tool_calls\"))\n",
    "            tool_response, city = handle_tool_call(msg)\n",
    "            messages.append(msg)\n",
    "            messages.append(tool_response)\n",
    "            response = ollama.chat(\n",
    "                model=MODEL_NAME,\n",
    "                messages=messages,\n",
    "                stream=False,\n",
    "                options={\n",
    "                    \"temperature\": 0.8\n",
    "                }\n",
    "            )\n",
    "\n",
    "            msg = response.get(\"message\", {})\n",
    "            \n",
    "        if msg.get(\"content\"):\n",
    "            return {\"role\": \"assistant\", \"content\": msg[\"content\"]}\n",
    "        \n",
    "        return {\"role\": \"assistant\", \"content\": \"I'm not sure how to respond.\"}\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(\"Exception: \", repr(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bab90d0-2b75-4530-81b4-26f27679af4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexender/Desktop/Projects/My_projects/envs/Torch_env/lib/python3.12/site-packages/gradio/chat_interface.py:328: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'messages', will be used.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chat Interface\n",
    "\n",
    "gr.ChatInterface(fn=chat, chatbot=gr.Chatbot(type=\"messages\")).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f51783d-5dce-459d-8c08-b3e9047b3296",
   "metadata": {},
   "source": [
    "### Multi Tool calling\n",
    "\n",
    "Even though the tool calling is working fine, we can call only one tools at a time since we are fetching the first tool\n",
    "```\n",
    "tool_call = message[\"tool_calls\"][0] # Since we are using only one tool we are getting the first one\n",
    "```\n",
    "from the ```handle_tool_call``` function and the LLM can call only one Tool at a time because of \n",
    "```\n",
    "if msg.get(\"tool_calls\"):\n",
    "```\n",
    "from ```chat``` function\n",
    "\n",
    "Now we are going to replace it with ```for and while``` loops for multiple tool calling option "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6badcd27-50ec-4392-bd56-b0c739e7f002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are handling all tool calls in a loop so its all handled\n",
    "\n",
    "def multi_handle_tool_calls(message):\n",
    "    responses = [] # using list to append all tools responses\n",
    "    cities = [] # using list to append all cities fetched from tool calls\n",
    "    for tool_call in message[\"tool_calls\"]:\n",
    "        arguments = tool_call[\"function\"][\"arguments\"]\n",
    "        if isinstance(arguments, str):\n",
    "            arguments = json.loads(arguments)\n",
    "        city = arguments.get('destination_city')\n",
    "        price = get_ticket_price(city)\n",
    "    \n",
    "        tool_call_id = tool_call.get(\"id\", str(uuid.uuid4()))\n",
    "        response = {\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": json.dumps({\"destination_city\": city, \"price\": price}),\n",
    "            \"tool_call_id\": tool_call_id\n",
    "        }\n",
    "        responses.append(response)\n",
    "        cities.append(city)\n",
    "    return responses, cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b335c423-8e5d-43f3-8e11-bf4c2a2a506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will make multiple calls\n",
    "\n",
    "def multi_chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    result = \"\"\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=MODEL_NAME,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            stream=False,\n",
    "            options={\n",
    "                \"temperature\": 0.8\n",
    "            }\n",
    "        )\n",
    "        print(response, \"\\n\")\n",
    "\n",
    "        msg = response.get(\"message\", {})\n",
    "        while msg.get(\"tool_calls\"): # this will allow the LLM to run all tool calls it makes\n",
    "            print(msg.get(\"tool_calls\"))\n",
    "            tool_response, city = multi_handle_tool_calls(msg)\n",
    "            messages.append(msg)\n",
    "            messages.extend(tool_response) # we are recieving a list of response so extend it instead of appending it\n",
    "            response = ollama.chat(\n",
    "                model=MODEL_NAME,\n",
    "                messages=messages,\n",
    "                stream=False,\n",
    "                options={\n",
    "                    \"temperature\": 0.8\n",
    "                }\n",
    "            )\n",
    "\n",
    "            msg = response.get(\"message\", {})\n",
    "            \n",
    "        if msg.get(\"content\"):\n",
    "            return {\"role\": \"assistant\", \"content\": msg[\"content\"]}\n",
    "        \n",
    "        return {\"role\": \"assistant\", \"content\": \"I'm not sure how to respond.\"}\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(\"Exception: \", repr(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83724a9d-da36-4577-b2bf-d8452dbb8e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexender/Desktop/Projects/My_projects/envs/Torch_env/lib/python3.12/site-packages/gradio/chat_interface.py:328: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'messages', will be used.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chat Interface\n",
    "\n",
    "gr.ChatInterface(fn=multi_chat, chatbot=gr.Chatbot(type=\"messages\")).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e52b53-1e43-43fb-b888-7bbbc00f405f",
   "metadata": {},
   "source": [
    "### Now we will connect the DB to fetch the values for the Tools to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fe3e26f4-6380-48bf-ab47-c768616b47be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "DB_HOST=\"localhost\"\n",
    "DB_PORT=\"3306\"\n",
    "DB_NAME=\"personal_DB\"\n",
    "DB_USER=\"Alex\"\n",
    "DB_PASSWORD=\"Alex@$14798|<</>>\"\n",
    "\n",
    "connection_string = (\n",
    "    f\"mysql+pymysql://{DB_USER}:{quote_plus(DB_PASSWORD)}@\"\n",
    "    f\"{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    ")\n",
    "\n",
    "engine = create_engine(connection_string, echo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c776c15d-977a-4456-a963-6d0b13b6d684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with engine.connect() as conn:\n",
    "#     result = conn.execute(text(\"select * from personal_DB.prices;\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8ffd0db6-3387-451e-8bfe-5b70f3db9f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ticket_price_db(city):\n",
    "    print(f\"DATABASE TOOL CALLED: Getting price for {city}\", flush=True)\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(f'SELECT price FROM personal_DB.prices WHERE city = \"{city.lower()}\"'))\n",
    "        result = result.fetchall()\n",
    "        conn.close()\n",
    "        return f\"Ticket price to {city} is ₹ {result[0][0]}\" if result else \"No price data available for this city\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "809ab35a-2060-4b79-ac50-6f755aeb243e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATABASE TOOL CALLED: Getting price for Chennai\n",
      "2025-11-30 19:11:54,539 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2025-11-30 19:11:54,539 INFO sqlalchemy.engine.Engine SELECT price FROM personal_DB.prices WHERE city = \"chennai\"\n",
      "2025-11-30 19:11:54,540 INFO sqlalchemy.engine.Engine [cached since 20.46s ago] {}\n",
      "2025-11-30 19:11:54,541 INFO sqlalchemy.engine.Engine ROLLBACK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ticket price to Chennai is ₹ 399.0'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ticket_price_db(\"Chennai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b1616a2b-2d55-407d-9797-1dd085230343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_handle_tool_calls_db(message):\n",
    "    responses = [] # using list to append all tools responses\n",
    "    cities = [] # using list to append all cities fetched from tool calls\n",
    "    for tool_call in message[\"tool_calls\"]:\n",
    "        if tool_call[\"function\"][\"name\"] == \"get_ticket_price\":\n",
    "            arguments = tool_call[\"function\"][\"arguments\"]\n",
    "            if isinstance(arguments, str):\n",
    "                arguments = json.loads(arguments)\n",
    "            city = arguments.get('destination_city')\n",
    "            price = get_ticket_price_db(city)\n",
    "        \n",
    "            tool_call_id = tool_call.get(\"id\", str(uuid.uuid4()))\n",
    "            response = {\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": json.dumps({\"destination_city\": city, \"price\": price}),\n",
    "                \"tool_call_id\": tool_call_id\n",
    "            }\n",
    "            responses.append(response)\n",
    "            cities.append(city)\n",
    "    return responses, cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8820205a-9682-491d-9bc0-28fc6407556b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_chat_db(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    result = \"\"\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=MODEL_NAME,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            stream=False,\n",
    "            options={\n",
    "                \"temperature\": 0.8\n",
    "            }\n",
    "        )\n",
    "        print(response, \"\\n\")\n",
    "\n",
    "        msg = response.get(\"message\", {})\n",
    "        while msg.get(\"tool_calls\"): # this will allow the LLM to run all tool calls it makes\n",
    "            print(msg.get(\"tool_calls\"))\n",
    "            tool_response, city = multi_handle_tool_calls_db(msg)\n",
    "            messages.append(msg)\n",
    "            messages.extend(tool_response) # we are recieving a list of response so extend it instead of appending it\n",
    "            response = ollama.chat(\n",
    "                model=MODEL_NAME,\n",
    "                messages=messages,\n",
    "                stream=False,\n",
    "                options={\n",
    "                    \"temperature\": 0.8\n",
    "                }\n",
    "            )\n",
    "\n",
    "            msg = response.get(\"message\", {})\n",
    "            \n",
    "        if msg.get(\"content\"):\n",
    "            return {\"role\": \"assistant\", \"content\": msg[\"content\"]}\n",
    "        \n",
    "        return {\"role\": \"assistant\", \"content\": \"I'm not sure how to respond.\"}\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(\"Exception: \", repr(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "874a3d8c-39d1-4b99-bbb1-5a0d2d2fa063",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexender/Desktop/Projects/My_projects/envs/Torch_env/lib/python3.12/site-packages/gradio/chat_interface.py:328: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'messages', will be used.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='qwen3' created_at='2025-11-30T14:18:21.481230367Z' done=True done_reason='stop' total_duration=24245003308 load_duration=6305198128 prompt_eval_count=226 prompt_eval_duration=3092597310 eval_count=70 eval_duration=14660346451 message=Message(role='assistant', content='Hello! How can I assist you today?', thinking='Okay, the user said \"hello\". I need to respond politely and briefly. Since there\\'s no specific query here, I should just greet them back and offer assistance. No need to call any functions because they didn\\'t ask for anything yet. Keep it friendly and open-ended.\\n', images=None, tool_calls=None) \n",
      "\n",
      "model='qwen3' created_at='2025-11-30T14:19:57.568050076Z' done=True done_reason='stop' total_duration=31791957382 load_duration=246691616 prompt_eval_count=253 prompt_eval_duration=894665865 eval_count=139 eval_duration=30538808831 message=Message(role='assistant', content='', thinking=\"Okay, the user is asking for ticket prices for Mumbai and Chennai. Let me check the tools available. There's a function called get_ticket_price that takes a destination_city parameter. Since the user mentioned two cities, I need to call this function twice, once for each city. Let me make sure to format the tool calls correctly in JSON within the XML tags. First, handle Mumbai, then Chennai. Alright, that should cover both cities.\\n\", images=None, tool_calls=[ToolCall(function=Function(name='get_ticket_price', arguments={'destination_city': 'mumbai'})), ToolCall(function=Function(name='get_ticket_price', arguments={'destination_city': 'chennai'}))]) \n",
      "\n",
      "[ToolCall(function=Function(name='get_ticket_price', arguments={'destination_city': 'mumbai'})), ToolCall(function=Function(name='get_ticket_price', arguments={'destination_city': 'chennai'}))]\n",
      "DATABASE TOOL CALLED: Getting price for mumbai\n",
      "2025-11-30 19:49:57,570 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2025-11-30 19:49:57,571 INFO sqlalchemy.engine.Engine SELECT price FROM personal_DB.prices WHERE city = \"mumbai\"\n",
      "2025-11-30 19:49:57,571 INFO sqlalchemy.engine.Engine [cached since 1970s ago] {}\n",
      "2025-11-30 19:49:57,573 INFO sqlalchemy.engine.Engine ROLLBACK\n",
      "DATABASE TOOL CALLED: Getting price for chennai\n",
      "2025-11-30 19:49:57,575 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2025-11-30 19:49:57,575 INFO sqlalchemy.engine.Engine SELECT price FROM personal_DB.prices WHERE city = \"chennai\"\n",
      "2025-11-30 19:49:57,576 INFO sqlalchemy.engine.Engine [cached since 2303s ago] {}\n",
      "2025-11-30 19:49:57,577 INFO sqlalchemy.engine.Engine ROLLBACK\n"
     ]
    }
   ],
   "source": [
    "# Chat Interface\n",
    "\n",
    "gr.ChatInterface(fn=multi_chat_db, chatbot=gr.Chatbot(type=\"messages\")).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc231ec3-d996-4c94-9783-a037457f9d38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
