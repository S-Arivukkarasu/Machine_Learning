{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e8a752e-d3b3-4d9b-a507-5721467d9d9f",
   "metadata": {},
   "source": [
    "# AI Tools\n",
    "\n",
    "## Project - Airlines AI Assistant\n",
    "\n",
    "We will make an AI Customer Support assistant for an Airline with pre-fixed prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9c24c46-cbff-4d87-ac33-5c67cd166f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Libraries\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import ollama\n",
    "from typing import List\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from bs4 import BeautifulSoup\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13194f55-b988-4b6f-8224-266f6d98737f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing System Messages and ollama details\n",
    "\n",
    "system_message = \"You are a helpful assistant for an Airline called FlightAI. \"\n",
    "system_message += \"Give short, courteous answers, no more than 1 sentence. \"\n",
    "system_message += \"Always be accurate. If you don't know the answer, say so.\"\n",
    "\n",
    "# system_message += \"\"\"\n",
    "# TOOLS INSTRUCTIONS:\n",
    "# - Only call a tool if the user explicitly asks about flight booking, ticket prices, or related information.\n",
    "# - If the user is greeting, chatting, or asking a general question, DO NOT call a tool. Just respond normally.\n",
    "# \"\"\"\n",
    "\n",
    "OLLAMA_URL = \"http://localhost:11434/api/chat\"\n",
    "MODEL_NAME=\"qwen3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a2c8966-615f-467f-87d4-8a46b8393f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat function for gradio\n",
    "\n",
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.8,\n",
    "        \"stream\": True  # Important: to get streaming responses\n",
    "    }\n",
    "    start_time = time.time()\n",
    "    response = requests.post(OLLAMA_URL, json=payload, stream=True)\n",
    "    result=\"\"\n",
    "    try:\n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                try:\n",
    "                    data = json.loads(line.decode('utf-8'))\n",
    "                    delta = data.get(\"message\", {}).get(\"content\", \"\")\n",
    "                    if delta:\n",
    "                        # Optional: remove unwanted <think> tags or others\n",
    "                        clean_delta = re.sub(r\"<think>.*?</think>\", \"\", delta, flags=re.DOTALL)\n",
    "                        result += clean_delta\n",
    "                        yield result\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "    finally:\n",
    "        # Ensure generator exits cleanly\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2fbfee6-2036-4491-80d1-2d7f0cf66442",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexender/Desktop/Projects/My_projects/envs/Torch_env/lib/python3.12/site-packages/gradio/chat_interface.py:328: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'messages', will be used.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradio UI\n",
    "\n",
    "gr.ChatInterface(fn=chat, chatbot=gr.Chatbot(type=\"messages\")).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63031c99-202f-4894-83ec-3e7db2e322dd",
   "metadata": {},
   "source": [
    "### Tools\n",
    "Tools are an incredibly powerful feature provided by the frontier LLMs.\n",
    "\n",
    "With tools, you can write a function, and have the LLM call that function as part of its response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f2054eb5-5bb5-4bfc-be7c-33e531f71b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by making a useful function\n",
    "\n",
    "ticket_prices = {\"chennai\": \"₹ 399\", \"mumbai\": \"₹ 899\", \"bengalore\": \"₹ 400\", \"hydrabad\": \"₹ 499\"}\n",
    "\n",
    "def get_ticket_price(destination_city):\n",
    "    print(f\"Tool get_ticket_price called for {destination_city}\")\n",
    "    city = destination_city.lower()\n",
    "    return ticket_prices.get(city, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "96cd79e0-b905-4925-95dc-ff261f815ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool get_ticket_price called for Chennai\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'₹ 399'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ticket_price(\"Chennai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ee145125-c6c1-4078-877c-5b60681eb36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's a particular dictionary structure that's required to describe our function:\n",
    "\n",
    "# Tool Name\n",
    "price_function = {\n",
    "        \"name\": \"get_ticket_price\",  # Name of the function\n",
    "        # This is important since its passed to LLM so that when it should call the tool we have to mention it clearly, its like system prompt for tools. Be clear as possible \n",
    "        \"description\": \"Get the price of a return ticket to the destination city. Call this whenever you need to know the ticket price, for example when a customer asks 'How much is a ticket to this city'\",\n",
    "        \"parameters\": {\n",
    "            # Here we have to give the details of the parameters, here we have one parameter so mentioning that here\n",
    "            \"type\": \"object\",\n",
    "            \"properties\":{\n",
    "                \"destination_city\":{\n",
    "                    # Mentioning the data type and where and how this input is used\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city that the customer wants to travel to\",\n",
    "                },\n",
    "            },\n",
    "            # we have to mention what are the must required parameters of the function(tool)\n",
    "            \"required\":[\"destination_city\"],\n",
    "            # It restricts the input object to allow only the explicitly defined properties — no extras.\n",
    "            # like only \"destination_city\": \"Berlin\" this is allowed they can't send extra prameters like \"class\": \"economy\"\n",
    "            \"additionalProperties\": False,\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "733c9a69-ca26-4813-8145-0098dd22dc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And this is included in a list of tools:\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",  # Since we are calling the function we have to mention it\n",
    "        \"function\": price_function,\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5332b60b-60c9-45ab-8133-4796c5a6a966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to write that function handle_tool_call:\n",
    "\n",
    "import uuid\n",
    "def handle_tool_call(message):\n",
    "    tool_call = message[\"tool_calls\"][0] # Since we are using only one tool we are getting the first one\n",
    "    arguments = tool_call[\"function\"][\"arguments\"]\n",
    "    if isinstance(arguments, str):\n",
    "        arguments = json.loads(arguments)\n",
    "    city = arguments.get('destination_city')\n",
    "    price = get_ticket_price(city)\n",
    "\n",
    "    tool_call_id = tool_call.get(\"id\", str(uuid.uuid4()))\n",
    "    response = {\n",
    "        \"role\": \"tool\",\n",
    "        \"content\": json.dumps({\"destination_city\": city, \"price\": price}),\n",
    "        \"tool_call_id\": tool_call_id\n",
    "    }\n",
    "    return response, city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f6f825c0-a60f-46ee-b616-7e9328a64f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tools we are going to use ollama package instead of APIs(Since APIs don't support tools)\n",
    "# This tools support will be provides by only specific models so we switch to Qwen from Mistral\n",
    "\n",
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    result = \"\"\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=MODEL_NAME,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            stream=False,\n",
    "            options={\n",
    "                \"temperature\": 0.8\n",
    "            }\n",
    "        )\n",
    "        print(response)\n",
    "\n",
    "        msg = response.get(\"message\", {})\n",
    "        if msg.get(\"tool_calls\"):\n",
    "            print(msg.get(\"tool_calls\"))\n",
    "            tool_response, city = handle_tool_call(msg)\n",
    "            messages.append(msg)\n",
    "            messages.append(tool_response)\n",
    "            response = ollama.chat(\n",
    "                model=MODEL_NAME,\n",
    "                messages=messages,\n",
    "                stream=False,\n",
    "                options={\n",
    "                    \"temperature\": 0.8\n",
    "                }\n",
    "            )\n",
    "\n",
    "            msg = response.get(\"message\", {})\n",
    "            \n",
    "        if msg.get(\"content\"):\n",
    "            return {\"role\": \"assistant\", \"content\": msg[\"content\"]}\n",
    "        \n",
    "        return {\"role\": \"assistant\", \"content\": \"I'm not sure how to respond.\"}\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(\"Exception: \", repr(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3bab90d0-2b75-4530-81b4-26f27679af4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexender/Desktop/Projects/My_projects/envs/Torch_env/lib/python3.12/site-packages/gradio/chat_interface.py:328: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'messages', will be used.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7870\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7870/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='qwen3' created_at='2025-08-30T16:32:37.815462099Z' done=True done_reason='stop' total_duration=15067125375 load_duration=51053623 prompt_eval_count=224 prompt_eval_duration=2581384020 eval_count=70 eval_duration=12428458945 message=Message(role='assistant', content='<think>\\nOkay, the user said \"hello\". I need to respond politely and briefly. Since there\\'s no specific query here, I should just greet them back and offer assistance. No need to call any functions because they didn\\'t ask for anything yet. Keep it friendly and open-ended.\\n</think>\\n\\nHello! How can I assist you today?', thinking=None, images=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# Chat Interface\n",
    "\n",
    "gr.ChatInterface(fn=chat, chatbot=gr.Chatbot(type=\"messages\")).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f72e478-6eb0-4cef-98b4-f49b154cdf50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
