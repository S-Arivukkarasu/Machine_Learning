{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1334619f-38a6-4a2c-95b1-28532ed0a20d",
   "metadata": {},
   "source": [
    "### BUT FIRST - Something cool - really showing you how \"model inference\" works using Hugging Face Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ef7ef0-141a-4351-9029-b3ab3a5592ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "class TokenPredictor:\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model_name = model_name\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            use_fast=True\n",
    "        )\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
    "            device_map=\"auto\"\n",
    "        ).eval()\n",
    "\n",
    "    def predict_tokens(self, prompt: str, max_tokens: int = 100) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Generate text token by token and track prediction probabilities.\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "\n",
    "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(self.device)\n",
    "\n",
    "        for _ in range(max_tokens):\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids=input_ids)\n",
    "\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # Top-3 predictions\n",
    "            top_probs, top_ids = torch.topk(probs, k=3)\n",
    "\n",
    "            top_probs = top_probs[0].tolist()\n",
    "            top_ids = top_ids[0].tolist()\n",
    "\n",
    "            tokens = [self.tokenizer.decode(t) for t in top_ids]\n",
    "\n",
    "            top_token = tokens[0]\n",
    "            top_prob = top_probs[0]\n",
    "\n",
    "            alternatives = [\n",
    "                (tokens[i], top_probs[i])\n",
    "                for i in range(1, len(tokens))\n",
    "            ]\n",
    "\n",
    "            predictions.append({\n",
    "                \"token\": top_token,\n",
    "                \"probability\": top_prob,\n",
    "                \"alternatives\": alternatives\n",
    "            })\n",
    "\n",
    "            # Append generated token\n",
    "            next_id = torch.tensor([[top_ids[0]]], device=self.device)\n",
    "            input_ids = torch.cat([input_ids, next_id], dim=-1)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "def create_token_graph(model_name: str, predictions: List[Dict]) -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Create a directed graph showing token predictions and alternatives.\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    G.add_node(\"START\", token=model_name, prob=\"START\", color=\"lightgreen\", size=4000)\n",
    "\n",
    "    for i, pred in enumerate(predictions):\n",
    "        token_id = f\"t{i}\"\n",
    "        G.add_node(\n",
    "            token_id,\n",
    "            token=pred[\"token\"],\n",
    "            prob=f\"{pred['probability'] * 100:.1f}%\",\n",
    "            color=\"lightblue\",\n",
    "            size=6000,\n",
    "        )\n",
    "\n",
    "        if i == 0:\n",
    "            G.add_edge(\"START\", token_id)\n",
    "        else:\n",
    "            G.add_edge(f\"t{i - 1}\", token_id)\n",
    "\n",
    "    last_id = None\n",
    "    for i, pred in enumerate(predictions):\n",
    "        parent_token = \"START\" if i == 0 else f\"t{i - 1}\"\n",
    "\n",
    "        for j, (alt_token, alt_prob) in enumerate(pred[\"alternatives\"]):\n",
    "            alt_id = f\"t{i}_alt{j}\"\n",
    "            G.add_node(\n",
    "                alt_id,\n",
    "                token=alt_token,\n",
    "                prob=f\"{alt_prob * 100:.1f}%\",\n",
    "                color=\"lightgray\",\n",
    "                size=6000,\n",
    "            )\n",
    "            G.add_edge(parent_token, alt_id)\n",
    "            last_id = parent_token\n",
    "\n",
    "    G.add_node(\"END\", token=\"END\", prob=\"100%\", color=\"red\", size=6000)\n",
    "    if last_id:\n",
    "        G.add_edge(last_id, \"END\")\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "def visualize_predictions(G: nx.DiGraph, figsize=(14, 80)):\n",
    "    \"\"\"\n",
    "    Visualize the token prediction graph with vertical layout and alternatives.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    pos = {}\n",
    "    spacing_y = 5\n",
    "    spacing_x = 5\n",
    "\n",
    "    main_nodes = [n for n in G.nodes() if \"_alt\" not in n]\n",
    "    for i, node in enumerate(main_nodes):\n",
    "        pos[node] = (0, -i * spacing_y)\n",
    "\n",
    "    for node in G.nodes():\n",
    "        if \"_alt\" in node:\n",
    "            main_token = node.split(\"_\")[0]\n",
    "            alt_num = int(node.split(\"_alt\")[1])\n",
    "            if main_token in pos:\n",
    "                x_offset = -spacing_x if alt_num == 0 else spacing_x\n",
    "                pos[node] = (x_offset, pos[main_token][1] + 0.05)\n",
    "\n",
    "    node_colors = [G.nodes[node][\"color\"] for node in G.nodes()]\n",
    "    node_sizes = [G.nodes[node][\"size\"] for node in G.nodes()]\n",
    "\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=node_sizes)\n",
    "    nx.draw_networkx_edges(G, pos, edge_color=\"gray\", arrows=True, arrowsize=20, alpha=0.7)\n",
    "\n",
    "    labels = {\n",
    "        node: f\"{G.nodes[node]['token']}\\n{G.nodes[node]['prob']}\"\n",
    "        for node in G.nodes()\n",
    "    }\n",
    "    nx.draw_networkx_labels(G, pos, labels, font_size=14)\n",
    "\n",
    "    plt.title(\"Token prediction (LLaMA 3.2 â€“ Hugging Face)\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    margin = 8\n",
    "    x_vals = [x for x, y in pos.values()]\n",
    "    y_vals = [y for x, y in pos.values()]\n",
    "    plt.xlim(min(x_vals) - margin, max(x_vals) + margin)\n",
    "    plt.ylim(min(y_vals) - margin, max(y_vals) + margin)\n",
    "\n",
    "    return plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0b396b-a263-48bc-a2d5-9f1f0c12b4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"In one sentence, describe the color orange to someone who has never been able to see\"\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "predictor = TokenPredictor(model_name)\n",
    "predictions = predictor.predict_tokens(message, max_tokens=30)\n",
    "\n",
    "G = create_token_graph(model_name, predictions)\n",
    "plt = visualize_predictions(G)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78c7287-4d84-4592-83cf-b521d6b156f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del predictor, predictions, G, plt\n",
    "gc.collect()\n",
    "torch.cuda.ipc_collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f86c30-83d5-48f6-b32f-e566d4e3a010",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802e7471-ec20-4793-a73b-e561f98972a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
