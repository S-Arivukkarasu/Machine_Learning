{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6394f738-5f22-41cb-831c-15b68be4378c",
   "metadata": {},
   "source": [
    "# Transformer Model Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4756569-1a2f-40a8-8d86-62de645fc5cc",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3959e25-dfe8-47c6-babf-4f512c564503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, AutoTokenizer, AutoModelForMaskedLM\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ac46775-34f7-471f-a4d1-48832ffc7055",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-cased\"\n",
    "\n",
    "model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "421b655a-83d4-43c8-940e-5e2bdf7411dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95d9f78b-6fd6-4250-94df-375b3140f575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ceccddda-1f94-4415-b591-78259de64824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa8a0acb-d9ad-4a6f-8318-ecbb60a64666",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"When life gives you lemons, don't make lemonade.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75e2c75-e6aa-4a1a-af70-a508a20b6d2f",
   "metadata": {},
   "source": [
    "* Tokenizer will tokenize the sentence here its sub-word level tokenization\n",
    "* Subword tokenization involves breaking words into smaller, meaningful subword units\n",
    "* Popular words dont get split, words rarely gets used usually gets breakdown "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fca36f50-caae-4197-b0a9-eb279dc97151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When',\n",
       " 'life',\n",
       " 'gives',\n",
       " 'you',\n",
       " 'lemon',\n",
       " '##s',\n",
       " ',',\n",
       " 'don',\n",
       " \"'\",\n",
       " 't',\n",
       " 'make',\n",
       " 'lemon',\n",
       " '##ade',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eb7304-a219-4d48-892c-747f4254befe",
   "metadata": {},
   "source": [
    "* Tokenizers break words based on the vocabulary its trained on\n",
    "* In this case its around 28996 which is the (word_embeddings): Embedding(28996, 768, padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e95a5ea-ba45-45d9-9f8d-4e20bed02e70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>token_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>##rica</td>\n",
       "      <td>15353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bang</td>\n",
       "      <td>12926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sales</td>\n",
       "      <td>15689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pathetic</td>\n",
       "      <td>18970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>##bro</td>\n",
       "      <td>12725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28991</th>\n",
       "      <td>January</td>\n",
       "      <td>1356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28992</th>\n",
       "      <td>fraud</td>\n",
       "      <td>10258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28993</th>\n",
       "      <td>م</td>\n",
       "      <td>589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28994</th>\n",
       "      <td>ט</td>\n",
       "      <td>543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28995</th>\n",
       "      <td>behaviour</td>\n",
       "      <td>9151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28996 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          tokens  token_id\n",
       "0         ##rica     15353\n",
       "1           Bang     12926\n",
       "2          Sales     15689\n",
       "3       pathetic     18970\n",
       "4          ##bro     12725\n",
       "...          ...       ...\n",
       "28991    January      1356\n",
       "28992      fraud     10258\n",
       "28993          م       589\n",
       "28994          ט       543\n",
       "28995  behaviour      9151\n",
       "\n",
       "[28996 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = tokenizer.vocab\n",
    "vocab_df = pd.DataFrame({\"tokens\": vocab.keys(), \"token_id\": vocab.values()})\n",
    "vocab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a4aed8e-4efe-4f6e-b1ed-bd991b15805f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[PAD]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[unused1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[unused2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[unused3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[unused4]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tokens\n",
       "token_id           \n",
       "0             [PAD]\n",
       "1         [unused1]\n",
       "2         [unused2]\n",
       "3         [unused3]\n",
       "4         [unused4]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_df = vocab_df.sort_values(by='token_id').set_index(\"token_id\")\n",
    "vocab_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1660f94-a556-4257-a3cc-25903081a3f8",
   "metadata": {},
   "source": [
    "* Now we encode this sentence to get the token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f22c7e2d-2ba9-4e94-a6d2-0812c0c30a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 1332,\n",
       " 1297,\n",
       " 3114,\n",
       " 1128,\n",
       " 22782,\n",
       " 1116,\n",
       " 117,\n",
       " 1274,\n",
       " 112,\n",
       " 189,\n",
       " 1294,\n",
       " 22782,\n",
       " 6397,\n",
       " 119,\n",
       " 102]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = tokenizer.encode(sentence)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "154d30e9-60a7-4a0d-8bd9-fd5f651cb3c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens), len(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4929e830-8350-4a36-b757-d54923f14eea",
   "metadata": {},
   "source": [
    "* These first and last token_ids in a sentence is special tokens in BERT\n",
    "* It denotes the start and the end of the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2365af2a-889e-4780-a4a0-c5c78dd5375c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tokens    [CLS]\n",
       " Name: 101, dtype: object,\n",
       " tokens    [SEP]\n",
       " Name: 102, dtype: object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_df.iloc[101], vocab_df.iloc[102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5823c07f-49b5-49fb-955b-50c35984b171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('When', 1332),\n",
       " ('life', 1297),\n",
       " ('gives', 3114),\n",
       " ('you', 1128),\n",
       " ('lemon', 22782),\n",
       " ('##s', 1116),\n",
       " (',', 117),\n",
       " ('don', 1274),\n",
       " (\"'\", 112),\n",
       " ('t', 189),\n",
       " ('make', 1294),\n",
       " ('lemon', 22782),\n",
       " ('##ade', 6397),\n",
       " ('.', 119)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(tokens, token_ids[1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9b3f19f-17db-4f56-b7d0-fe60c7c7eb1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] When life gives you lemons, don ' t make lemonade. [SEP]\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(token_ids=token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b7a7a2-e2f3-456e-8bdf-8e535163cde6",
   "metadata": {},
   "source": [
    "* input_ids - token_ids which is the input for the model\n",
    "* token_type_ids - used in pre training of the model i.e to find which is question and which is context in a Q/A input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82759dbf-9268-4d47-b122-c9e9125f83b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1332, 1297, 3114, 1128, 22782, 1116, 117, 1274, 112, 189, 1294, 22782, 6397, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_out = tokenizer(sentence)\n",
    "token_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cbfe2dc-f65b-47e2-bfb5-8084c8c726ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When life gives you lemons, make lemonade.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence2 = sentence.replace(\"don't \", \"\")\n",
    "sentence2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5c2480-0025-471e-8778-8d4d942fb519",
   "metadata": {},
   "source": [
    "* padding - used to add extra tokens if 2 or more sentence is given to match the shape of the input matrix but those extra tokens should not interfer in our final prediction\n",
    "* attention_mask - it will tell the model which token is to be taken as input and which should not\n",
    "* (position_embeddings): Embedding(512, 768) - 512 is the no of tokens(300 words) we can give as a input to the model at once, 768 is the size of the vector which input tokens will be converted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6094246-02a5-4574-a7e1-1538a6a80d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1332, 1297, 3114, 1128, 22782, 1116, 117, 1274, 112, 189, 1294, 22782, 6397, 119, 102], [101, 1332, 1297, 3114, 1128, 22782, 1116, 117, 1294, 22782, 6397, 119, 102, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_out2 = tokenizer([sentence, sentence2], padding=True)\n",
    "token_out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1793a07-feb6-48bc-9eb0-454ad9abcc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] When life gives you lemons, don ' t make lemonade. [SEP]\n",
      "[CLS] When life gives you lemons, make lemonade. [SEP] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(token_out2['input_ids'][0]))\n",
    "print(tokenizer.decode(token_out2['input_ids'][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e721cf8e-8f8a-4182-907b-5e8dc7fee93d",
   "metadata": {},
   "source": [
    "## Word Embbeding\n",
    "\n",
    "#### Now we will encode a new sentence and give it to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7d70c1b-3a11-4ad0-ac3a-104396254b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1706, 6378, 3708, 1143, 1142, 4268,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Tokenize me this please\"\n",
    "\n",
    "encoded_inputs = tokenizer(text=text, return_tensors='pt')\n",
    "encoded_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006cd78b-0fef-472a-b2a9-7da6d1a779e4",
   "metadata": {},
   "source": [
    "* Since model is pytorch model it will expect a tensor input rather than dict or list\n",
    "* return_tensors - It will return the output as tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9283ec00-fca1-4d4b-94ed-7c9f6aed074b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**encoded_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e20cd6-ddc0-456d-9257-1bf4462b4bb4",
   "metadata": {},
   "source": [
    "* last_hidden_state - Refers the final ouput token for main layer for each input token\n",
    "* pooler_output - Refers the final ouput of the entire sentence not the individual tokens, its like a summarization of the entire input sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cde22bc2-a756-48b7-af53-950d364a46c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_state = output.last_hidden_state\n",
    "pooler_output = output.pooler_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8985a98e-3520-4388-8d2e-01507858edc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 768])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32d3d5c6-804c-44d0-b4f0-ad9417b22e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ec97b4c-31df-4a90-9a04-dffb5ac41ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['To', '##ken', '##ize', 'me', 'this', 'please']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74ba484f-9d31-414a-9323-62078af205df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    encoded_inputs = tokenizer(text=text, return_tensors='pt')\n",
    "    return model(**encoded_inputs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c759866e-c3b7-48c2-9306-bd4142c4ccab",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"There was a fly drinking from my soup\"\n",
    "text2 = \"To become a commercial pilot, he had to fly for 1500 hours\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65eeba7d-4813-451a-805f-3b257ad754d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['There', 'was', 'a', 'fly', 'drinking', 'from', 'my', 'soup'],\n",
       " ['To',\n",
       "  'become',\n",
       "  'a',\n",
       "  'commercial',\n",
       "  'pilot',\n",
       "  ',',\n",
       "  'he',\n",
       "  'had',\n",
       "  'to',\n",
       "  'fly',\n",
       "  'for',\n",
       "  '1500',\n",
       "  'hours'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token1 = tokenizer.tokenize(text1)\n",
    "token2 = tokenizer.tokenize(text2)\n",
    "\n",
    "token1, token2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3dda9084-da60-4ea0-825b-2e905207cce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = predict(text=text1)\n",
    "out2 = predict(text=text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aad30a1c-a270-42b5-a4a6-40d1e3236e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 768]), torch.Size([1, 768]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb1 = out1[0:, token1.index(\"fly\"), :].detach()\n",
    "emb2 = out2[0:, token2.index(\"fly\"), :].detach()\n",
    "\n",
    "emb1.shape, emb2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d1ff75f1-e458-425e-9317-3cbd847ffdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a08e4961-8706-43f4-9019-d59974ba5213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.41355014)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(emb1[0], emb2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5061f422-a8e5-4f69-9f9a-9df6ca885530",
   "metadata": {},
   "source": [
    "## Masked Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1eb398-406c-4e5d-bc4a-d95e42e00623",
   "metadata": {},
   "source": [
    "* MLM is the pre training objective of the models like BERT\n",
    "* We have a seperate masking models and we import it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "11036bb0-8d57-44d6-b4bf-52f3894e2a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=28996, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "mlm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ad1388-392f-4389-ad87-c32d699e2632",
   "metadata": {},
   "source": [
    "* Creating a mask from tokenizer and adding it into a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e6e9cb63-4acb-4814-8f5b-3e6a35d3534a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[MASK]'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the masked tokens\n",
    "\n",
    "mask = tokenizer.mask_token\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c29f33f4-bb7f-4bef-be65-203cd5d0c23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I want to [MASK] pizza for tonight'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_sentence = f\"I want to {mask} pizza for tonight\"\n",
    "mask_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c906c51-fafe-459f-bdae-dfc52fa90f67",
   "metadata": {},
   "source": [
    "* Tokenizing that sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9cda19e-5364-4d07-b4bd-3e081d2d0822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'want', 'to', '[MASK]', 'pizza', 'for', 'tonight']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_token = tokenizer.tokenize(mask_sentence)\n",
    "mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "68025bcf-5e87-48e6-9f55-272c4a50f069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,   146,  1328,  1106,   103, 13473,  1111,  3568,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_encoded_input = tokenizer(mask_sentence, return_tensors='pt')\n",
    "mask_encoded_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d55464-95b8-4da7-9d47-443e0e237bad",
   "metadata": {},
   "source": [
    "* Feeding that sentence to the model\n",
    "* It will return a multi dimensional vector, in that logits tensor will have the masked value details\n",
    "* Logits represent raw, un-normalized scores that the model assigns to each potential words to fill in the masked positions\n",
    "* Logits will contain index of the word embedding values and the values will be the probabilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ed2bca51-dc4b-4784-b7d0-ff25d651c4d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=None, logits=tensor([[[ -7.4283,  -7.2895,  -7.4779,  ...,  -6.2929,  -5.9589,  -6.4331],\n",
       "         [ -7.9286,  -8.2635,  -8.0442,  ...,  -6.6752,  -6.4446,  -6.8911],\n",
       "         [-12.3447, -11.9961, -12.7443,  ...,  -8.4030,  -6.5324,  -8.1336],\n",
       "         ...,\n",
       "         [ -9.1358,  -8.8955,  -8.9833,  ...,  -7.8610,  -5.0709,  -8.3300],\n",
       "         [ -9.4683,  -9.5075,  -9.0676,  ...,  -6.7674,  -6.1865,  -7.4156],\n",
       "         [-14.4339, -14.6208, -14.9550,  ..., -11.6409, -11.3482, -13.2990]]],\n",
       "       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_output = mlm_model(**mask_encoded_input)\n",
    "mask_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c1cfbdb7-fc4c-49bd-9556-593688047e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 28996)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_logits = mask_output.logits.detach().numpy()[0]\n",
    "mask_logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344fc053-018e-4601-bb9a-ff08021be592",
   "metadata": {},
   "source": [
    "* Fetching the masked value using index\n",
    "* These will contain the values(probability) of the word embbedings of length `28996` which BERT is trained on\n",
    "* Converting those values into probabilities with the softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f525668b-fb17-491e-a4f4-6770f28daaee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.731374, -6.393911, -6.147725, ..., -5.651458, -3.668856,\n",
       "       -4.999485], shape=(28996,), dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_logits = mask_logits[mask_token.index(mask)+1]\n",
    "masked_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7fe86e71-614d-4ce1-afed-e3ae6c70c738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7fd41b68-8315-4437-8dca-b43d3399c9d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.6420100e-10, 5.1038607e-10, 6.5285466e-10, ..., 1.0723631e-09,\n",
       "       7.7870954e-09, 2.0582136e-09], shape=(28996,), dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confidence_score = softmax(masked_logits)\n",
    "confidence_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8e471118-b2d9-4113-b722-d67bf642e201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.99999994)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confidence_score.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48921b56-fb65-4608-ab38-c3260e92ca6d",
   "metadata": {},
   "source": [
    "* Sorting it by index and fetching the top 5 values which has he highest probabilites\n",
    "* Decoding those 5 values will give the masked word which the model predicted from it's vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4235d7be-82f6-46a8-8411-33a65741d724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1138, 3940, 1243, 1294, 1546])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(confidence_score)[::-1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "320d973a-4da2-4c3e-b07e-fc1a06a1fb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25416425 have\n",
      "I want to have pizza for tonight\n",
      "0.17271347 eat\n",
      "I want to eat pizza for tonight\n",
      "0.15204962 get\n",
      "I want to get pizza for tonight\n",
      "0.11082382 make\n",
      "I want to make pizza for tonight\n",
      "0.08149549 order\n",
      "I want to order pizza for tonight\n"
     ]
    }
   ],
   "source": [
    "for i in np.argsort(confidence_score)[::-1][:5]:\n",
    "    predict_tokens = tokenizer.decode(i)\n",
    "    score = confidence_score[i]\n",
    "\n",
    "    print(score, predict_tokens)\n",
    "    print(mask_sentence.replace(mask, predict_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797f8b37-568d-473b-b531-028bdf100302",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
