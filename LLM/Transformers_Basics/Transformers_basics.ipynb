{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3959e25-dfe8-47c6-babf-4f512c564503",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexender/Desktop/Projects/My_projects/envs/Torch_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, AutoTokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ac46775-34f7-471f-a4d1-48832ffc7055",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf.co/bert-base-cased/1d8bdcee6021e2c25f0325e84889b61c2eb26b843eef5659c247af138d64f050?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1741791250&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MTc5MTI1MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9iZXJ0LWJhc2UtY2FzZWQvMWQ4YmRjZWU2MDIxZTJjMjVmMDMyNWU4NDg4OWI2MWMyZWIyNmI4NDNlZWY1NjU5YzI0N2FmMTM4ZDY0ZjA1MD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=u3rVhyMowx0cjaX%7EYSF90RwVj3jJemRGc7GW3bj8WnNnDvG%7EACdw-HJWrSlz91wIioLex40SgkJqkOtzXAvVgkyPv1DCt7P9rULV2JhWpwuWIDAlFfQyOG1edMTjESHalOcor18wEYEJ2CLCQu--EiNlWvtdqq7gAq4Yc3bmgyL6tmT%7Erk%7E%7EQA2kwNwkjprNtVKaF5kg3JqR3dndl-rgjdk49PBvCfWtzRollZyjLEqcp5HNNuMwhocEUOZJTkHzDjRDJcPKWeSDjzt4vN4qxJAUxHnvIhKFpMtSHJ8VVS1Iu8cecurxhiLtURZUazvmeli784rnITsnAu-RflvTJg__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-cased\"\n",
    "\n",
    "model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "421b655a-83d4-43c8-940e-5e2bdf7411dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95d9f78b-6fd6-4250-94df-375b3140f575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceccddda-1f94-4415-b591-78259de64824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa8a0acb-d9ad-4a6f-8318-ecbb60a64666",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"When life gives you lemons, don't make lemonade.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fca36f50-caae-4197-b0a9-eb279dc97151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When',\n",
       " 'life',\n",
       " 'gives',\n",
       " 'you',\n",
       " 'lemon',\n",
       " '##s',\n",
       " ',',\n",
       " 'don',\n",
       " \"'\",\n",
       " 't',\n",
       " 'make',\n",
       " 'lemon',\n",
       " '##ade',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e95a5ea-ba45-45d9-9f8d-4e20bed02e70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>token_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>##xing</td>\n",
       "      <td>24118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cheyenne</td>\n",
       "      <td>19095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Judiciary</td>\n",
       "      <td>25932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hometown</td>\n",
       "      <td>9694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>maneuvers</td>\n",
       "      <td>27559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28991</th>\n",
       "      <td>attend</td>\n",
       "      <td>4739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28992</th>\n",
       "      <td>Loyola</td>\n",
       "      <td>23828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28993</th>\n",
       "      <td>snout</td>\n",
       "      <td>18924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28994</th>\n",
       "      <td>##ić</td>\n",
       "      <td>7733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28995</th>\n",
       "      <td>Iris</td>\n",
       "      <td>13476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28996 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          tokens  token_id\n",
       "0         ##xing     24118\n",
       "1       Cheyenne     19095\n",
       "2      Judiciary     25932\n",
       "3       hometown      9694\n",
       "4      maneuvers     27559\n",
       "...          ...       ...\n",
       "28991     attend      4739\n",
       "28992     Loyola     23828\n",
       "28993      snout     18924\n",
       "28994       ##ić      7733\n",
       "28995       Iris     13476\n",
       "\n",
       "[28996 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = tokenizer.vocab\n",
    "vocab_df = pd.DataFrame({\"tokens\": vocab.keys(), \"token_id\": vocab.values()})\n",
    "vocab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a4aed8e-4efe-4f6e-b1ed-bd991b15805f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[PAD]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[unused1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[unused2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[unused3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[unused4]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tokens\n",
       "token_id           \n",
       "0             [PAD]\n",
       "1         [unused1]\n",
       "2         [unused2]\n",
       "3         [unused3]\n",
       "4         [unused4]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_df = vocab_df.sort_values(by='token_id').set_index(\"token_id\")\n",
    "vocab_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f22c7e2d-2ba9-4e94-a6d2-0812c0c30a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 1332,\n",
       " 1297,\n",
       " 3114,\n",
       " 1128,\n",
       " 22782,\n",
       " 1116,\n",
       " 117,\n",
       " 1274,\n",
       " 112,\n",
       " 189,\n",
       " 1294,\n",
       " 22782,\n",
       " 6397,\n",
       " 119,\n",
       " 102]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = tokenizer.encode(sentence)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "154d30e9-60a7-4a0d-8bd9-fd5f651cb3c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 16)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens), len(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2365af2a-889e-4780-a4a0-c5c78dd5375c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokens    [CLS]\n",
       "Name: 101, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_df.iloc[101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30c0bd37-fa82-49de-a815-b7eed588eef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokens    [SEP]\n",
       "Name: 102, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_df.iloc[102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5823c07f-49b5-49fb-955b-50c35984b171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('When', 1332),\n",
       " ('life', 1297),\n",
       " ('gives', 3114),\n",
       " ('you', 1128),\n",
       " ('lemon', 22782),\n",
       " ('##s', 1116),\n",
       " (',', 117),\n",
       " ('don', 1274),\n",
       " (\"'\", 112),\n",
       " ('t', 189),\n",
       " ('make', 1294),\n",
       " ('lemon', 22782),\n",
       " ('##ade', 6397),\n",
       " ('.', 119)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(tokens, token_ids[1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d9b3f19f-17db-4f56-b7d0-fe60c7c7eb1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] When life gives you lemons, don ' t make lemonade. [SEP]\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(token_ids=token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "82759dbf-9268-4d47-b122-c9e9125f83b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1332, 1297, 3114, 1128, 22782, 1116, 117, 1274, 112, 189, 1294, 22782, 6397, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_out = tokenizer(sentence)\n",
    "token_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7cbfe2dc-f65b-47e2-bfb5-8084c8c726ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When life gives you lemons, make lemonade.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence2 = sentence.replace(\"don't \", \"\")\n",
    "sentence2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c6094246-02a5-4574-a7e1-1538a6a80d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1332, 1297, 3114, 1128, 22782, 1116, 117, 1274, 112, 189, 1294, 22782, 6397, 119, 102], [101, 1332, 1297, 3114, 1128, 22782, 1116, 117, 1294, 22782, 6397, 119, 102, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_out2 = tokenizer([sentence, sentence2], padding=True)\n",
    "token_out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a1793a07-feb6-48bc-9eb0-454ad9abcc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] When life gives you lemons, don ' t make lemonade. [SEP]\n",
      "[CLS] When life gives you lemons, make lemonade. [SEP] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(token_out2['input_ids'][0]))\n",
    "print(tokenizer.decode(token_out2['input_ids'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51e56fe-3be2-4cdc-af8d-5776dcd2bf7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
