{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbcc5759-2c28-4c10-86eb-cd79fcff8531",
   "metadata": {},
   "source": [
    "# Implementing DeepSpeed: A Hands-On Approach üöÄ\n",
    "\n",
    "## Learning Objectives üéØ\n",
    "- Learn how to implement DeepSpeed to optimize training for large models.\n",
    "- Understand the configuration of DeepSpeed for memory-efficient training.\n",
    "- Gain hands-on experience with DeepSpeed, even on a single GPU, to grasp the key concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff03a61f-7b13-4d4e-9c35-5218eaf52c8e",
   "metadata": {},
   "source": [
    "## Installing Axolotl and DeepSpeed üõ†Ô∏è\n",
    "Install Axolotl with DeepSpeed support. While DeepSpeed is optimized for multi-GPU setups, you can still run this configuration on a single GPU to understand how the system works and the benefits of the Zero Redundancy Optimizer (ZeRO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4230562-3408-48c5-a0a6-dc2046b1e245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --no-build-isolation axolotl[flash-attn,deepspeed]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7e0f23-4d2d-48b0-bec8-ab13e6fb8b14",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ef64f60-752a-4278-83eb-76f73d85968e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a0c957-6657-4568-a79a-46b4993a845b",
   "metadata": {},
   "source": [
    "## Configuration Setup for DeepSpeed Training üìù\n",
    "Set up a YAML configuration for training the model, using a smaller model for free-tier compatibility. The original model from the lesson can be used if you have access to more powerful hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "915f8ab6-542c-4edb-a250-c190cda02218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "train_config = {\n",
    "    # \"base_model\": \"unsloth/Meta-Llama-3.1-8B-Instruct\", # The original model from the lesson\n",
    "    \"base_model\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", # For faster loading on Colab\n",
    "\n",
    "\n",
    "    # dataset params\n",
    "    \"datasets\": [\n",
    "        {\n",
    "            \"path\": \"Arivukkarasu/squad_for_llms\",\n",
    "            \"type\": {\n",
    "                \"system_prompt\": \"Read the following context and concisely answer my question.\",\n",
    "                \"field_system\": \"system\",\n",
    "                \"field_instruction\": \"question\",\n",
    "                \"field_input\": \"context\",\n",
    "                \"field_output\": \"output\",\n",
    "                \"format\": \"<|user|> {input} {instruction} </s> <|assistant|>\",\n",
    "                \"no_input_format\": \"<|user|> {instruction} </s> <|assistant|>\",\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    "    \"output_dir\": \"./models/Llama3_squad\",\n",
    "\n",
    "    # model params\n",
    "    \"sequence_length\": 2048,\n",
    "    \"bf16\": \"auto\",\n",
    "    \"tf32\": False,\n",
    "\n",
    "    # training params\n",
    "    \"micro_batch_size\": 4,\n",
    "    \"num_epochs\": 4,\n",
    "    \"optimizer\": \"adamw_bnb_8bit\",\n",
    "    \"learning_rate\": 0.0002,\n",
    "    \"logging_steps\": 1,\n",
    "\n",
    "    # LoRA / qLoRA\n",
    "    \"adapter\": \"lora\",\n",
    "    \"lora_r\": 32,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"lora_target_linear\": True,\n",
    "\n",
    "    # Gradient Accumulation\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "\n",
    "    # Gradient Checkpointing\n",
    "    \"gradient_checkpointing\": True,\n",
    "}\n",
    "\n",
    "\n",
    "# Write the YAML file\n",
    "with open(\"deepspeed_train.yml\", 'w') as file:\n",
    "    yaml.dump(train_config, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8adc08d-89a2-41d5-b8a6-95883d742ee8",
   "metadata": {},
   "source": [
    "## DeepSpeed Configuration üß†\n",
    "Create a DeepSpeed configuration (Zero Stage 1) to enable memory optimization during training. This configuration will reduce memory usage and allow larger batch sizes, especially beneficial when scaling to multiple GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c2e7241-8d5f-44a2-a39b-19bdea8c7524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "zero1_conf = {\n",
    "    \"zero_optimization\": {\"stage\": 1, \"overlap_comm\": True},\n",
    "    \"bf16\": {\"enabled\": \"auto\"},\n",
    "    \"fp16\": {\n",
    "        \"enabled\": \"auto\",\n",
    "        \"auto_cast\": False,\n",
    "        \"loss_scale\": 0,\n",
    "        \"initial_scale_power\": 32,\n",
    "        \"loss_scale_window\": 1000,\n",
    "        \"hysteresis\": 2,\n",
    "        \"min_loss_scale\": 1,\n",
    "    },\n",
    "    \"gradient_accumulation_steps\": \"auto\",\n",
    "    \"gradient_clipping\": \"auto\",\n",
    "    \"train_batch_size\": \"auto\",\n",
    "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "    \"wall_clock_breakdown\": False,\n",
    "}\n",
    "\n",
    "with open(\"zero1.json\", 'w') as fp:\n",
    "  json.dump(zero1_conf, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aea96d2-7a11-4bf5-81e7-3048ee5d04e8",
   "metadata": {},
   "source": [
    "## Launching DeepSpeed Training üöÄ\n",
    "Launch the training using the `accelerate launch` command with DeepSpeed enabled. While running this on a single GPU won't show the full benefits, it will still provide the learning experience and understanding of how DeepSpeed optimizes large-scale training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a58f0bcd-0800-4b11-8078-b9cc8b4a99b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !accelerate launch -m axolotl.cli.train deepspeed_train.yml --deepspeed zero1.json\n",
    "# Optional: Merge the trained adapter\n",
    "# !accelerate launch -m axolotl.cli.merge_lora deepspeed_train.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70351bab-b48e-46dd-b978-a3324e15b545",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
