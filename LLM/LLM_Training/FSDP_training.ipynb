{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c05809c-608d-4ae9-9f96-8066278ab58b",
   "metadata": {},
   "source": [
    "# Applying FSDP(Fully Sharded Data Parallel): Real-World Usage and Best Practices üöÄ\n",
    "\n",
    "## Learning Objectives üéØ\n",
    "- Learn how to apply Fully Sharded Data Parallel (FSDP) for large-scale models.\n",
    "- Understand FSDP‚Äôs configurations for optimizing memory usage and efficiency.\n",
    "- Gain hands-on experience by running FSDP even on a single GPU, keeping in mind that the real benefits become clear with multiple GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35243360-94c2-45a6-be80-e77b40f37f55",
   "metadata": {},
   "source": [
    "## Installing Axolotl and DeepSpeed üõ†Ô∏è\n",
    "Install Axolotl with DeepSpeed support. While DeepSpeed is optimized for multi-GPU setups, you can still run this configuration on a single GPU to understand how the system works and the benefits of the Zero Redundancy Optimizer (ZeRO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0f54ccf-7c07-4be4-98cf-213153ad32d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --no-build-isolation axolotl[flash-attn,deepspeed]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1662e37a-9919-418e-9ff5-826a4863d123",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8246afd-afd4-42ac-9283-1ec3db116ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176e77ec-e0e2-4fd5-a5b9-45557a88ac89",
   "metadata": {},
   "source": [
    "## Applying FSDP Configurations üß†\n",
    "FSDP uses configurations like `full_shard` and `auto_wrap` to distribute memory across GPUs efficiently. Even though we‚Äôre running this on a single GPU, you‚Äôll understand how the memory is handled and prepared for larger-scale distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fcc53bd-526c-4609-a5da-46a3a8f19326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "train_config = {\n",
    "    # \"base_model\": \"casperhansen/llama-3-70b-fp16\", # will only work on at least 2 x 24GB Gpus\n",
    "    \"base_model\": \"unsloth/llama-3-8b-Instruct\",\n",
    "\n",
    "    # dataset params\n",
    "    \"datasets\": [{\"path\": \"Yukang/LongAlpaca-12k\", \"type\": \"alpaca\"}],\n",
    "    \"output_dir\": \"./models/llama70B-LongAlpaca\",\n",
    "\n",
    "    # model params\n",
    "    \"sequence_length\": 1024,\n",
    "    \"pad_to_sequence_len\": True,\n",
    "    \"special_tokens\": {\"pad_token\": \"<|end_of_text|>\"},\n",
    "\n",
    "    \"bf16\": \"auto\",\n",
    "    \"tf32\": False,\n",
    "\n",
    "    # training params\n",
    "    \"micro_batch_size\": 1,\n",
    "    \"num_epochs\": 1,\n",
    "    \"optimizer\": \"adamw_torch\",\n",
    "    \"learning_rate\": 0.0002,\n",
    "\n",
    "    \"logging_steps\": 1,\n",
    "\n",
    "    # LoRA / qLoRA\n",
    "    \"adapter\": \"qlora\",\n",
    "    \"lora_r\": 32,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"lora_target_linear\": True,\n",
    "\n",
    "    # Gradient Accumulation\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "\n",
    "    # Gradient Checkpointing\n",
    "    \"gradient_checkpointing\": True,\n",
    "\n",
    "    # Low Precision\n",
    "    \"load_in_8bit\": False,\n",
    "    \"load_in_4bit\": True,\n",
    "\n",
    "    # Flash Attention\n",
    "    \"flash_attention\": False,\n",
    "\n",
    "    # FSDP\n",
    "    \"fsdp\": [\"full_shard\", \"auto_wrap\"],\n",
    "    \"fsdp_config\": {\n",
    "        \"fsdp_offload_params\": True,\n",
    "        \"fsdp_cpu_ram_efficient_loading\": True,\n",
    "        \"fsdp_state_dict_type\": \"FULL_STATE_DICT\",\n",
    "        \"fsdp_transformer_layer_cls_to_wrap\": \"LlamaDecoderLayer\",\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Write the YAML file\n",
    "with open(\"fsdp_train.yml\", 'w') as file:\n",
    "    yaml.dump(train_config, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad67fd13-3719-45ae-b271-330ab7df3409",
   "metadata": {},
   "source": [
    "## Launching FSDP Training üöÄ\n",
    "Start the training using FSDP with `accelerate launch`. FSDP works best in a multi-GPU setup, but you can still proceed with a single GPU to observe how it manages memory sharding and learn its benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec696549-eb54-4bbd-827c-a9edbf4a2dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !accelerate launch -m axolotl.cli.train fsdp_train.yml\n",
    "# Optional: Merge the trained adapter\n",
    "# !accelerate launch -m axolotl.cli.merge_lora train_fsdp.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c371967-9a5d-4a9f-ae1e-95b678c06c37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
