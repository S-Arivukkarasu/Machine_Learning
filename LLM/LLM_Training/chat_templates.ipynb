{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "827381fc-c096-41e9-b87b-19421313ee80",
   "metadata": {},
   "source": [
    "# Chat Templates: Hands-On Overview üìò\n",
    "\n",
    "## Learning Objectives üéØ\n",
    "- Understand how to install specific versions of the `transformers` library.\n",
    "- Learn the basics of using chat templates with language models.\n",
    "- Explore the functionality of the AutoTokenizer for managing conversation flow.\n",
    "\n",
    "## Introduction\n",
    "In this notebook, we will delve into the use of chat templates which aid in structuring conversations for language models, ensuring more coherent and context-aware responses.\n",
    "\n",
    "## Installation of Transformers Library\n",
    "Before we start, we need to ensure that the correct version of the `transformers` library is installed. This step is crucial as different versions may have different compatibilities with the models and methods we intend to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2ae0e5f-3392-46fd-9d2b-4155d65c1bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a specific version of the transformers package\n",
    "# !pip install transformers==4.41.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02250697-0174-41ba-ac16-ddfdbc5c6023",
   "metadata": {},
   "source": [
    "**Importing Required Modules**\n",
    "\n",
    "Once the necessary library is installed, we need to import the tokenizer which will help us in processing text for the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b75be7d0-9269-4c83-9359-f30e849cdae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec9a86b-8702-43fe-9a02-5cac1520d433",
   "metadata": {},
   "source": [
    "**Setting Up Chat Templates**\n",
    "\n",
    "Chat templates are essential for structuring the input and output in a conversation with a language model. They help in maintaining context and generating relevant responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf989fc9-132c-446d-809a-29c4fec536f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define example messages in a conversation\n",
    "messages = [\n",
    "   {\"role\": \"user\", \"content\": \"How do chat templates work?\"},\n",
    "   {\"role\": \"assistant\", \"content\": \"Chat templates help  LLMs like me generate more coherent responses by providing a structured way to organize the conversation.\"},\n",
    "   {\"role\": \"user\", \"content\": \"How do I use them?\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9b1570-1434-4c34-b07f-75a023baabea",
   "metadata": {},
   "source": [
    "**Tokenizing and Applying Chat Templates**\n",
    "\n",
    "We will now use a tokenizer from the transformers library to process these messages and apply chat templates to them. This will prepare our data for interaction with a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "000a832b-16d6-430f-8705-b465537d347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "blender_tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n",
    "mistral_tokenizer = AutoTokenizer.from_pretrained(\"unsloth/mistral-7b-instruct-v0.2\")\n",
    "gemma_tokenizer = AutoTokenizer.from_pretrained(\"unsloth/gemma-7b-it\")\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(\"unsloth/llama-3-8b-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c625b88b-a2df-414d-a2da-394226890ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<s> [INST] How do chat templates work? [/INST] Chat templates help  LLMs like me generate more coherent responses by providing a structured way to organize the conversation.</s> [INST] How do I use them? [/INST]\n",
      "\n",
      "<bos><start_of_turn>user\n",
      "How do chat templates work?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Chat templates help  LLMs like me generate more coherent responses by providing a structured way to organize the conversation.<end_of_turn>\n",
      "<start_of_turn>user\n",
      "How do I use them?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "How do chat templates work?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Chat templates help  LLMs like me generate more coherent responses by providing a structured way to organize the conversation.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "How do I use them?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(blender_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
    "# Blenderbot is not a chat template model. Use .generate() directly instead of chat methods.\n",
    "print()\n",
    "print(mistral_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
    "print()\n",
    "print(gemma_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
    "print()\n",
    "print(llama_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805cf8bb-7974-43b0-a5c5-71b746e13793",
   "metadata": {},
   "source": [
    "**Conclusion üìù**\n",
    "\n",
    "In this notebook, we have successfully installed and utilized the transformers library to apply chat templates for structuring conversations with language models. This approach is fundamental in enhancing the quality and relevance of responses generated by LLMs, thereby making interactions more meaningful.\n",
    "\n",
    "Feel free to experiment with different models and templates to see how the responses vary and what suits your application best. Happy experimenting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cd6ffe-dbce-4c9c-8b78-43e98b8c2dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
