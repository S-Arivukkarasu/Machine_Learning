{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df02f934-d2b0-4c5c-a1b6-d913eafe2913",
   "metadata": {},
   "source": [
    "# Generation: An Interactive Guide ðŸŒŸ\n",
    "\n",
    "## Learning Objectives ðŸŽ¯\n",
    "- Learn to install and use the transformers library.\n",
    "- Understand the initialization and use of models for generating text.\n",
    "- Explore interactive text generation with a pre-trained model.\n",
    "\n",
    "## Setup and Installation\n",
    "Make sure to set the notebook to use a GPU runtime to enhance performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81036cec-1597-4208-a9a2-4db554edd25f",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "Import necessary libraries for causal language modeling and authentication for accessing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29d76b71-9dbb-4835-a7ce-c20df01dc22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4cddb0-a0ba-4dad-b2fa-469fad11c100",
   "metadata": {},
   "source": [
    "## Authentication\n",
    "Authenticate to access restricted models from Hugging Face. Replace 'your_token_here' with your actual Hugging Face API token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6844e3e-023d-49ff-b119-c88f67a57178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfab7d38-a3a5-4ec0-87d7-c4e3513bd4f5",
   "metadata": {},
   "source": [
    "## Model Setup\n",
    "Set the device to GPU and load the model and tokenizer. Specify the model and tokenizer names, load them, and configure the device settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f18287e-7014-4f0d-88eb-de47deceee88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92bd7834-4bad-4720-8eba-f5641853cb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26bdf4de-b699-4767-b96e-1bc10e3bec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "918c8de6-37b5-4ea8-b225-721ad18ffba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"head_dim\": 64,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 2048,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 5632,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"mlp_bias\": false,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 22,\n",
       "  \"num_key_value_heads\": 4,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.49.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba6b138-1433-41f1-bcc5-aaa613bcfff4",
   "metadata": {},
   "source": [
    "## Interactive Text Generation\n",
    "Setup the conversation and prepare inputs for the model. Define a chat template with user and assistant roles, encode the messages, and apply the chat template.\n",
    "\n",
    "We will use the same chat message for this also and add_generation_prompt will be set as True to tell the model the next sentence should be generated by the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6484164d-2479-47c5-b2d8-3a12a86c7025",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define example messages in a conversation\n",
    "messages = [\n",
    "   {\"role\": \"user\", \"content\": \"How do chat templates work?\"},\n",
    "   {\"role\": \"assistant\", \"content\": \"Chat templates help  LLMs like me generate more coherent responses by providing a structured way to organize the conversation.\"},\n",
    "   {\"role\": \"user\", \"content\": \"How do I use them?\"},\n",
    "]\n",
    "\n",
    "model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors='pt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1187dbe6-7c9d-4c4d-8b63-7e1c06616847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  529, 29989,  1792, 29989, 29958,    13,  5328,   437, 13563, 17475,\n",
       "           664, 29973,     2, 29871,    13, 29966, 29989,   465, 22137, 29989,\n",
       "         29958,    13,  1451,   271, 17475,  1371, 29871,   365, 26369, 29879,\n",
       "           763,   592,  5706,   901, 16165,   261,   296, 20890,   491, 13138,\n",
       "           263,  2281,  2955,   982,   304,  2894,   675,   278, 14983, 29889,\n",
       "             2, 29871,    13, 29966, 29989,  1792, 29989, 29958,    13,  5328,\n",
       "           437,   306,   671,   963, 29973,     2, 29871,    13, 29966, 29989,\n",
       "           465, 22137, 29989, 29958,    13]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92865958-eba7-4f79-8b26-b819fadd2e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 75])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e0cadf-7535-4227-9d1c-7035e3286ae8",
   "metadata": {},
   "source": [
    "It's still the token ID's from the Tokenizer but structured in a different way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0b6dc13-d4f3-479c-bb81-bc70e2f3729e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "How do chat templates work?</s> \n",
      "<|assistant|>\n",
      "Chat templates help  LLMs like me generate more coherent responses by providing a structured way to organize the conversation.</s> \n",
      "<|user|>\n",
      "How do I use them?</s> \n",
      "<|assistant|>\n",
      "To use chat templates, you can follow these steps:\n",
      "\n",
      "1. Log in to your LLM chatbot platform.\n",
      "2. Click on the \"Templates\" tab in the left-hand menu.\n",
      "3. Click on the \"Add Template\" button.\n",
      "4. Choose the template you want to use from the list of available templates.\n",
      "5. Customize the template by adding your own text, images, and other elements.\n",
      "6. Save your template and assign it to a conversation.\n",
      "\n",
      "Once you have created a chat template, you can use it to generate responses to common customer queries or to customize responses based on the conversation.</s>\n"
     ]
    }
   ],
   "source": [
    "generated = model.generate(model_inputs)\n",
    "decoded_response = tokenizer.batch_decode(generated)\n",
    "\n",
    "print(decoded_response[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd24e82-2759-44d7-a39e-4aacc6d7f039",
   "metadata": {},
   "source": [
    "**Here the model clearly hallucinated and response is not correct but it's able to generate reponses**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c442fe7-e083-459a-bc74-014efc8319cf",
   "metadata": {},
   "source": [
    "## Generate and Decode Responses\n",
    "Generate responses using the model and decode them. Adjust parameters like `max_new_tokens`, `do_sample`, `top_p`, `temperature`, and `top_k` to explore different generation settings.\n",
    "\n",
    "We need to add some parameters to control the response of the model\n",
    "- `max_new_token` - It will allow the model to generate the new token upto some numbers, from the above input we already used 75 tokens if we set 10 it will generate 10 extra tokens but it will not contain useful info whatever the model generates it will stop/truncate at the 10th token, this llama version has only 2048 tokens per sequence so we have to limit within that \n",
    "- `do_sample` - If we change the values of max_new_token every time the output will be static so we will enable the do_sample, it will allow the model to generate answers differently but within the max token limit\n",
    "- `temperature` - A parameter that controls the randomness and creativity of generated text. A higher temperature (e.g., 0.8-1.0) introduces more randomness, leading to diverse and potentially more creative outputs, while a lower temperature (e.g., 0.0-0.4) results in more predictable and deterministic outputs.\n",
    "- `top_p` - top_p (also known as nucleus sampling) is a parameter that controls the diversity of the generated text. It sets a threshold for the cumulative probability distribution of token choices, essentially determining how many of the most probable tokens the model considers when generating the next word. A lower top_p value (e.g., 0.1) restricts the model to a smaller, more focused pool of likely tokens, leading to more predictable and coherent output. Conversely, a higher top_p value (e.g., 0.9) allows the model to consider a wider range of possibilities, potentially leading to more diverse and creative, but possibly less coherent, outputs. Usually top_p should be high.\n",
    "- `top_k` - top_k is a parameter that restricts the model's next token selection to the top k most probable tokens. This helps control the diversity and predictability of the generated text. A smaller k value leads to more predictable and constrained outputs, while a larger k allows for more variety and randomness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b910b15e-b2e1-4286-8f42-2daa2598fad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(model_inputs, max_new_tokens=128, do_sample=True, top_p=0.9, temperature=1.0, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6eba771f-4ca3-44e2-97fd-6264bed4dffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "How do chat templates work?</s> \n",
      "<|assistant|>\n",
      "Chat templates help  LLMs like me generate more coherent responses by providing a structured way to organize the conversation.</s> \n",
      "<|user|>\n",
      "How do I use them?</s> \n",
      "<|assistant|>\n",
      "Sure, here's how you can use chat templates to improve your conversational skills with chatbots:\n",
      "\n",
      "1. Create a template for a basic conversation: Start by creating a basic template that covers the most common topics and questions. For example, you could include questions like \"How can I order a drink?\" or \"Can you recommend a good restaurant nearby?\"\n",
      "\n",
      "2. Customize the template: Once you have a basic template, customize it as needed. For instance, if you want to add more questions about the drink, you can add them. Similarly, if you want to add more options for restaur\n"
     ]
    }
   ],
   "source": [
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "print(decoded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96347a25-6eb8-49cd-9382-fd43a14f0a0b",
   "metadata": {},
   "source": [
    "## Conclusion ðŸ“˜\n",
    "This notebook provided an interactive guide to text generation using the transformers library and a pre-trained model. Through hands-on examples, we explored the power of language models in generating context-aware text based on predefined templates. Experiment with different models and parameters to see how they affect the generated text's style and coherence. Enjoy your journey into NLP!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
