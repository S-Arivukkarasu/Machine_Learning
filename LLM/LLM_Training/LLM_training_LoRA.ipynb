{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca947d91-6be3-4a74-a281-4ab275c43fa2",
   "metadata": {},
   "source": [
    "# Fitting Giants: Practical Introduction to LoRA for Large Models üöÄ\n",
    "\n",
    "## Learning Objectives üéØ\n",
    "- Understand the hardware requirements necessary to train large models.\n",
    "- Install specific versions of dependencies to maintain consistency across training environments.\n",
    "- Configure and execute training sessions for large-scale models using advanced settings.\n",
    "- Explore techniques like LoRA to enhance model performance without increasing computational costs prohibitively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5b8ffd-d9f7-443d-bd29-ca2032e0554e",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd1a4294-d07d-49e2-9bb5-e82f2072eed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f9ea41-8874-49fe-b487-b207d953789c",
   "metadata": {},
   "source": [
    "## Library Installation üõ†Ô∏è\n",
    "Install the Axolotl library from a specified GitHub commit to ensure that all participants use the same library version, promoting consistency and reliability in the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fce0196e-bb13-4a2f-9a40-2f211d2ec861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --no-build-isolation axolotl[flash-attn,deepspeed]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd76ea3-d46d-4697-ae58-26cd5647f59a",
   "metadata": {},
   "source": [
    "## Configuration of Training Parameters üìù\n",
    "Set up a YAML configuration for training large models. This setup will detail all necessary parameters, including the base model, dataset specifics, and advanced options like batch sizes and learning rates, tailored to handle the demands of large-scale model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad96dcee-494f-4f61-9493-b205cece29a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "train_config = \"\"\"\n",
    "# model params\n",
    "base_model: unsloth/Meta-Llama-3.1-8B-Instruct\n",
    "\n",
    "# dataset params\n",
    "datasets:\n",
    "  - path: jaydenccc/AI_Storyteller_Dataset\n",
    "    type:\n",
    "      system_prompt: \"You are an amazing storyteller. From the following synopsis, create an engaging story.\"\n",
    "      field_system: system\n",
    "      field_instruction: synopsis\n",
    "      field_output: short_story\n",
    "      format: \"<|user|>\\n {instruction} </s>\\n<|assistant|>\"\n",
    "      no_input_format: \"<|user|> {instruction} </s>\\n<|assistant|>\"\n",
    "\n",
    "output_dir: ./models/Llama3_Storyteller2\n",
    "\n",
    "\n",
    "# model params\n",
    "sequence_length: 512\n",
    "bf16: auto\n",
    "tf32: false\n",
    "\n",
    "# training params\n",
    "micro_batch_size: 1\n",
    "num_epochs: 1\n",
    "optimizer: adamw_bnb_8bit\n",
    "learning_rate: 0.0002\n",
    "\n",
    "logging_steps: 1\n",
    "\n",
    "\n",
    "# LoRA\n",
    "adapter: lora\n",
    "\n",
    "lora_r: 16\n",
    "lora_alpha: 16\n",
    "lora_dropout: 0.05\n",
    "\n",
    "lora_target_linear: true\n",
    "\n",
    "# Gradient Accumulation\n",
    "gradient_accumulation_steps: 1\n",
    "\n",
    "# Gradient Checkpointing\n",
    "gradient_checkpointing: true\n",
    "\"\"\"\n",
    "\n",
    "# Convert the YAML string to a Python dictionary\n",
    "yaml_dict = yaml.safe_load(train_config)\n",
    "\n",
    "\n",
    "# Write the YAML file\n",
    "with open(\"advanced_train.yml\", 'w') as file:\n",
    "    yaml.dump(yaml_dict, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0af7ea1-6152-4dbf-9bf3-30c56bbd0d43",
   "metadata": {},
   "source": [
    "## Launching the Training Session üöÄ\n",
    "Initiate the training process with an `accelerate launch` command tailored for large models. This session will utilize significant GPU resources, reflecting the practical challenges and solutions in training large models efficiently.\n",
    "\n",
    "Axolotl will train only on the small matrices in the model i.e. only a selected parameters so we nned to merge the trained parameters with the model and axolotl.cli.merge_lora will merge the trained parameters to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e00bbaa-3cb2-4334-a723-91739c76fa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !accelerate launch -m axolotl.cli.train advanced_train.yml\n",
    "# Optional: Merge the trained adapter\n",
    "# !accelerate launch -m axolotl.cli.merge_lora advanced_train.yml\n",
    "\n",
    "# Since Llama 3 is bigger model training with Llama 2\n",
    "# training this in colab "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea32e089-2f9e-4cd9-9952-9409646b2b5c",
   "metadata": {},
   "source": [
    "## Initializing Text Generation Pipeline üöÄ\n",
    "Set up a text generation pipeline using a pre-trained model. This pipeline will utilize a specific transformer model configured for generating narrative text, showcasing how advanced models can be employed directly in practical applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04abc5be-dc33-4c64-9aae-34d3a557bdb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35b63923c8d84d6fb1af36c9e44e0211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# pipe = pipeline(\"text-generation\", model=\"TheFuzzyScientist/Llama3_Storyteller\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "# Loading into cpu since GPU is very small\n",
    "pipe = pipeline(\"text-generation\", model=\"TheFuzzyScientist/Llama3_Storyteller\", torch_dtype=torch.bfloat16, device_map=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5035a9d5-435f-48a0-90dc-74792e0101cf",
   "metadata": {},
   "source": [
    "## Preparing and Generating Text üìù\n",
    "Prepare a prompt for text generation using custom messages tailored to test the storytelling capabilities of the model. Generate text based on this prompt to evaluate the model's creative output and the effectiveness of LoRA adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "869806ff-fd28-41a1-ad58-d3f62872e187",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\":\"system\", \"content\": \"You are an amazing storyteller. From the following synopsis, create an engaging story.\"},\n",
    "    {\"role\": \"user\", \"content\": \"A bright student was working with The Fuzzy Scientist on a project.\"},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24a6df24-04d5-4632-8bd7-f989e588f573",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062a5220-dbb7-4afb-963b-278885566667",
   "metadata": {},
   "source": [
    "## Reviewing Generated Output üïµÔ∏è‚Äç‚ôÇÔ∏è\n",
    "\n",
    "\n",
    "Analyze the generated text to assess how well the model with integrated LoRA adapters performs in real-world storytelling tasks. This step is crucial for understanding the enhancements provided by LoRA in practical scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88c4947d-c4ab-402f-8f34-f458cc98ce20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are an amazing storyteller. From the following synopsis, create an engaging story.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "A bright student was working with The Fuzzy Scientist on a project.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "It was a typical day in the small, cluttered laboratory of The Fuzzy Scientist, a brilliant and eccentric inventor known for his unorthodox approach to science. The room was filled with strange contraptions, beakers filled with bubbling liquids, and an assortment of gadgets that defied explanation. Amidst the chaos, a bright and curious student named Emma sat at a workbench, surrounded by papers and notes, working on a project with The Fuzzy Scientist.\n",
      "\n",
      "Emma had always been fascinated by science and had been lucky enough to land an internship with The Fuzzy Scientist, who was renowned for his groundbreaking discoveries in the field of\n"
     ]
    }
   ],
   "source": [
    "outputs = pipe(prompt, max_new_tokens=128)\n",
    "\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c85d23-a6c1-4851-86d8-090966c80d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
