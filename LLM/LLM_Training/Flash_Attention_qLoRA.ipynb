{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0ea2466-9c52-4643-8afc-0b7b397b623b",
   "metadata": {},
   "source": [
    "# Edge of Hardware Limits: Scaling Inputs with Flash Attention 2 üöÄ\n",
    "\n",
    "## Learning Objectives üéØ\n",
    "- Understand the hardware requirements for scaling large models and using advanced techniques like Flash Attention.\n",
    "- Explore how Flash Attention 2 can optimize large input processing, provided the correct GPU architecture (Ampere or newer) is available.\n",
    "- Configure and launch a specialized training session while staying within hardware limits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160dde51-2b43-453e-8286-2ee7f8745175",
   "metadata": {},
   "source": [
    "## Library Installation üõ†Ô∏è\n",
    "Install the necessary libraries, including Flash Attention 2, which will enable more efficient handling of large inputs. Ensure you are using an Ampere GPU for this step, as Flash Attention only works with these newer architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bc2b82e-95d1-41e2-86bc-0668defa023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --no-build-isolation axolotl[flash-attn,deepspeed]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268b42ca-5106-471a-99a6-2033e9338481",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c27a4dbb-ceea-4f27-85ea-b19855e9ab72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b75dbf-ad30-4b9b-b490-10593451e25b",
   "metadata": {},
   "source": [
    "## Configuration Setup for Large-Scale Training üìù\n",
    "Set up the YAML configuration for training large models with extended input lengths. In this case, we are using the \"unsloth/gemma-2-27b-it\" model, which requires more advanced GPU capabilities for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80a526bd-fa69-4225-a967-60dccd99e6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "train_config = {\n",
    "    # \"base_model\": \"unsloth/gemma-2-27b-it-bnb-4bit\", # the 27B doesn't fit on the free tier, use it if you have access to a 24GB GPU\n",
    "    \"base_model\": \"unsloth/gemma-2-9b-it-bnb-4bit\",\n",
    "\n",
    "     # dataset params\n",
    "    \"datasets\": [{\"path\": \"Yukang/LongAlpaca-12k\", \"type\": \"alpaca\"}],\n",
    "    \"output_dir\": \"./models/LongAlpaca\",\n",
    "\n",
    "    # model params\n",
    "    \"sequence_length\": 1024,\n",
    "    \"bf16\": \"auto\",\n",
    "    \"tf32\": False,\n",
    "\n",
    "    # training params\n",
    "    \"micro_batch_size\": 1,\n",
    "    \"num_epochs\": 1,\n",
    "    \"optimizer\": \"adamw_bnb_8bit\",\n",
    "    \"learning_rate\": 0.0002,\n",
    "\n",
    "    \"logging_steps\": 1,\n",
    "\n",
    "    # LoRA / qLoRA\n",
    "    \"adapter\": \"qlora\",\n",
    "    \"lora_r\": 32,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"lora_target_linear\": True,\n",
    "\n",
    "    # Gradient Accumulation\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "\n",
    "    # Gradient Checkpointing\n",
    "    \"gradient_checkpointing\": True,\n",
    "\n",
    "    # Low Precision\n",
    "    \"load_in_8bit\": False,\n",
    "    \"load_in_4bit\": True,\n",
    "\n",
    "    # Flash Attention\n",
    "    \"flash_attention\": False,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Write the YAML file\n",
    "with open(\"specialised_train_flash.yml\", 'w') as file:\n",
    "    yaml.dump(train_config, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c0438e-42be-45ec-922e-a4b991e3cb8c",
   "metadata": {},
   "source": [
    "## Training Launch üöÄ\n",
    "Start the training process using the `accelerate launch` command. This will initiate the training with large-scale inputs and specialized configurations like Flash Attention 2, taking full advantage of an Ampere GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8298b6d0-09e8-403c-ba1e-e88a3ea9d361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !accelerate launch -m axolotl.cli.train specialised_train.yml\n",
    "# # Optional: Merge the trained adapter\n",
    "# !accelerate launch -m axolotl.cli.merge_lora specialised_train.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e22fc9-59e8-41a8-a4cb-e749776ffb30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
