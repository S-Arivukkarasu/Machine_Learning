{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "340c5985-3650-4782-82d7-03f96b2d7044",
   "metadata": {},
   "source": [
    "# Level-Up Giants: 8-bit Training for Massive Models üöÄ\n",
    "\n",
    "## Learning Objectives üéØ\n",
    "- Understand the hardware requirements for training large-scale models.\n",
    "- Learn to install specialized libraries for advanced model training.\n",
    "- Configure training parameters effectively for large models using YAML.\n",
    "- Explore techniques like 8-bit optimization to manage VRAM usage efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10996245-06ce-41d6-b051-71b9cddbe3e2",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85d3826f-3b4f-4a76-a3a1-eb0402288d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cf3870-fad0-4f32-9f21-f0fd8ab1fc8d",
   "metadata": {},
   "source": [
    "## Library Installation üõ†Ô∏è\n",
    "Install the Axolotl library from a specified GitHub commit to ensure that all participants use the same library version, promoting consistency and reliability in the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd3dec3f-bf03-400d-8928-7755efc3b5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --no-build-isolation axolotl[flash-attn,deepspeed]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eac4abd-9f31-4f05-961e-805ac35f9e4c",
   "metadata": {},
   "source": [
    "## Training Configuration with YAML üìù\n",
    "Set up a detailed training configuration using YAML. This configuration will specify model parameters and training options that are designed to maximize efficiency on limited hardware by using techniques such as gradient checkpointing and 8-bit loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31e9fd03-ff13-4fad-931f-38519a7ccd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "train_config = {\n",
    "    # \"base_model\": \"microsoft/Phi-3-meidum-128k-instruct\" # this requires a 24GB video card\n",
    "    \"base_model\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", # using smaller model to speed up training, same concepts apply\n",
    "\n",
    "    # dataset params\n",
    "    \"datasets\": [\n",
    "        {\n",
    "            \"path\": \"Arivukkarasu/squad_for_llms\",\n",
    "            \"type\": {\n",
    "                \"system_prompt\": \"Read the following context and concisely answer my question.\",\n",
    "                \"field_system\": \"system\",\n",
    "                \"field_instruction\": \"question\",\n",
    "                \"field_input\": \"context\",\n",
    "                \"field_output\": \"output\",\n",
    "                \"format\": \"<|user|> {input} {instruction} </s> <|assistant|>\",\n",
    "                \"no_input_format\": \"<|user|> {instruction} </s> <|assistant|>\",\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    "    \"output_dir\": \"./models/\",\n",
    "\n",
    "    # model params\n",
    "    \"sequence_length\": 2048,\n",
    "\n",
    "    \"bf16\": \"auto\",\n",
    "    \"tf32\": False,\n",
    "\n",
    "    # training params\n",
    "    \"micro_batch_size\": 4,\n",
    "    \"num_epochs\": 1,\n",
    "    \"optimizer\": \"adamw_bnb_8bit\",\n",
    "    \"learning_rate\": 0.0002,\n",
    "\n",
    "    \"logging_steps\": 1,\n",
    "\n",
    "    # LoRA\n",
    "    \"adapter\": \"lora\",\n",
    "    \"lora_r\": 32,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"lora_target_linear\": True,\n",
    "\n",
    "    # Gradient Accumulation\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "\n",
    "    # Gradient Checkpointing\n",
    "    \"gradient_checkpointing\": True,\n",
    "\n",
    "    # Low Precision\n",
    "    \"load_in_8bit\": True,\n",
    "\n",
    "    # Train on Inputs\n",
    "    \"train_on_inputs\": False,\n",
    "}\n",
    "\n",
    "\n",
    "# Write the YAML file\n",
    "with open(\"specialised_train.yml\", 'w') as file:\n",
    "    yaml.dump(train_config, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e66c6d-ea68-4fa3-865e-e6d4408d5e3c",
   "metadata": {},
   "source": [
    "## Initiate Model Training üöÄ\n",
    "Begin the training process using an optimized setup. This includes using 8-bit precision and other settings that help in managing VRAM usage, making it feasible to run the training on GPUs with less memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "264bc67f-b6e1-408f-ac1a-d2a4a7a7cfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !accelerate launch -m axolotl.cli.train specialised_train.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11ff515a-5fd3-4090-9c13-5829b279cc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional: Merge the trained adapter\n",
    "# !accelerate launch -m axolotl.cli.merge_lora specialised_train.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0921c71-81fd-4060-8ae1-34da933ed954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
